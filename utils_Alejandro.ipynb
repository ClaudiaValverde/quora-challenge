{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe60a52a-1797-4d1f-9a9f-b294910bf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import spacy \n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "531f3ca3-c915-4be7-b45a-ceef26e21b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "def preproces_train_data_Word2Vect(df, save=True):\n",
    "    #Drop nans\n",
    "    df.dropna(inplace=True)\n",
    "    #Build training data for Word2vect:\n",
    "    #First separate question marks and words using regular expressions\n",
    "    pattern = r\"(\\w+|[?!.])\"\n",
    "    sentences = list(np.append(df['question1'].values,df['question2'].values))\n",
    "    sentences = [' '.join(re.findall(pattern, sent)) for sent in sentences]\n",
    "    brief_cleaning = (re.sub(\"[^\\w'?]+\", ' ', str(row)).lower() for row in sentences)\n",
    "    \n",
    "    #Import spacy \n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    \n",
    "    #Use the spacy pipeline separating words etc\n",
    "    txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000,  n_process=-1)]\n",
    "    df_clean = pd.DataFrame({'clean': txt})\n",
    "    df_clean = df_clean.dropna().drop_duplicates()\n",
    "    if save:\n",
    "        df_clean.to_csv('clean_w2v_data.csv', index=False)\n",
    "    return df_clean\n",
    "\n",
    "def preproces_data_Word2Vect(df):\n",
    "    string_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    out = []\n",
    "    for col in string_columns:\n",
    "        sentences = list(df[col].values)\n",
    "        #First separate question marks and words using regular expressions\n",
    "        pattern = r\"(\\w+|[?!.])\"\n",
    "        sentences = list(np.append(df['question1'].values,df['question2'].values))\n",
    "        sentences = [' '.join(re.findall(pattern, sent)) for sent in sentences]\n",
    "        brief_cleaning = (re.sub(\"[^\\w'?]+\", ' ', str(row)).lower() for row in sentences)\n",
    "        \n",
    "        #Import spacy \n",
    "        nlp = spacy.load('en_core_web_sm') \n",
    "        \n",
    "        #Use the spacy pipeline separating words etc\n",
    "        txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000,  n_process=-1)]\n",
    "        out.append(txt)\n",
    "    return np.array(out)\n",
    "\n",
    "def train_Word2Vect(df_clean=pd.read_csv('clean_w2v_data.csv'), num_features = 300, num_epochs = 20,\n",
    "                    min_word_count = 0, num_workers = multiprocessing.cpu_count(), context_size = 5, \n",
    "                    downsampling = 1e-3, seed = 1, sg = 0, save=True):\n",
    "    \n",
    "    word2vec = w2v.Word2Vec(\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    vector_size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    "    )\n",
    "    \n",
    "    word2vec.build_vocab(sent, progress_per=10000)\n",
    "    word2vec.train(sent, total_examples=word2vec.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "    if save:\n",
    "        word2vec.save(\"word2vec.model\")\n",
    "    return word2vec.wv\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"^a-zA-Z\", \" \", raw)\n",
    "    clean = clean.lower()\n",
    "    words = clean.split()\n",
    "    return words\n",
    "    \n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    word_list = sentence_to_wordlist(sentence)\n",
    "    word_vectors = []\n",
    "    for w in word_list:\n",
    "        if w in word2vec.key_to_index.keys():\n",
    "            word_vectors.append(word2vec[w])\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def get_Word2Vect(df, word2vec=Word2Vec.load(\"word2vec.model\").wv, phrase2vec=doc_to_vec):\n",
    "    data = preproces_data_Word2Vect(df, save=False)\n",
    "    out = []\n",
    "    if len(data.shape) >1:\n",
    "        for val in data[:,0]:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = np.array(out)\n",
    "        for i in range(1, data.shape[1]):\n",
    "            out = []\n",
    "            for val in data[:,i]:\n",
    "                out.append(doc_to_vec(val,word2vec))\n",
    "            out = np.array(out)\n",
    "            array = np.stack((array,out))\n",
    "    else:\n",
    "        for val in data:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = np.array(out)\n",
    "    return array\n",
    "\n",
    "def get_Word2Vect_from_clean(df, word2vec=Word2Vec.load(\"word2vec.model\").wv, phrase2vec=doc_to_vec):\n",
    "    data = df.values\n",
    "    out = []\n",
    "    if len(data.shape) >1:\n",
    "        for val in data[:,0]:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = np.array(out)\n",
    "        for i in range(1, data.shape[1]):\n",
    "            out = []\n",
    "            for val in data[:,i]:\n",
    "                out.append(doc_to_vec(val,word2vec))\n",
    "            out = np.array(out)\n",
    "            array = np.stack((array,out))\n",
    "    else:\n",
    "        for val in data:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = np.array(out)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632bf7d-ee6c-46be-9530-509506d1262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "df = pd.read_csv(\"../nlp_deliv1_materials/quora_train_data.csv\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1998f105-0de5-420d-bc91-85781a384fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('clean_w2v_data.csv')\n",
    "word2vec = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0178a8f-38d3-4f09-a7c5-981ada4f5778",
   "metadata": {},
   "source": [
    "Here is an example using a pre trained word to vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f734da-ee74-4dd0-940d-f41221fa8821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18908401,  0.14575334, -0.24050768, ..., -0.21747333,\n",
       "         0.08525033,  0.17485468],\n",
       "       [ 0.1206454 ,  0.1098704 ,  0.171647  , ..., -0.09448619,\n",
       "        -0.08320801,  0.0506992 ],\n",
       "       [-0.42076406,  0.17165159,  0.25065342, ..., -0.0953576 ,\n",
       "         0.03478063,  0.1543766 ],\n",
       "       ...,\n",
       "       [-0.219928  , -0.029034  , -0.1162586 , ..., -0.072036  ,\n",
       "        -0.038287  ,  0.24115232],\n",
       "       [ 0.241744  ,  0.04891361,  0.0080844 , ...,  0.028048  ,\n",
       "        -0.2740474 ,  0.479464  ],\n",
       "       [-0.03195641, -0.10047483,  0.014281  , ..., -0.03649074,\n",
       "        -0.0140646 ,  0.21292908]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_Word2Vect_from_clean(df_clean, word2vec=KeyedVectors.load(\"pretrained.model\"), phrase2vec=doc_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff3a1720-9f90-4bf2-aef3-94ac3cf5d996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 415665, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_Word2Vect(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf23ee7b-30e1-4435-92fd-1b73d7ac9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "pre = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d4299cf-1191-4f76-941f-47eef8a69af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.save(\"pretrained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a77ae0ad-f101-4842-a406-2c57b4667773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20577545,  0.12746239, -0.04717724, -0.02534887,  0.04027225,\n",
       "        0.152412  ,  0.03551663, -0.03253975,  0.08493976, -1.6322538 ,\n",
       "        0.21031661, -0.1983688 , -0.11061212,  0.08063625, -0.05675174,\n",
       "       -0.07887324, -0.12003312, -0.15204847,  0.21590875,  0.13083738,\n",
       "        0.21441011,  0.40429187,  0.27155975, -0.01802613, -0.27786648,\n",
       "       -0.00815025,  0.26502863, -0.16491945,  0.08708687, -0.01988237,\n",
       "        0.06162221,  0.17012987, -0.3386976 , -0.19019699, -1.0490575 ,\n",
       "        0.12061688, -0.23097901, -0.086456  ,  0.00959194,  0.04805613,\n",
       "       -0.08873121, -0.27393562, -0.06129425, -0.12130875,  0.184645  ,\n",
       "        0.13793913,  0.21891037,  0.13277763,  0.00689214,  0.02593046,\n",
       "        0.10792162, -0.14590938,  0.00507363, -0.09312012, -0.23943488,\n",
       "        0.39449978,  0.07809716, -0.09254488,  0.07620325,  0.08252013,\n",
       "        0.4164734 , -0.090637  ,  0.07884928,  0.46675876, -0.14334024,\n",
       "       -0.24197625,  0.1956075 ,  0.09268922,  0.157938  , -0.13387226,\n",
       "        0.04115876, -0.04514462,  0.00606438,  0.45864874,  0.07632375,\n",
       "       -0.14317614,  0.00227937,  0.2105791 , -0.13032913, -0.15482712,\n",
       "       -0.09600829,  0.06206788,  0.3980264 , -0.14632386,  0.00395313,\n",
       "        0.11418659, -0.27903688,  0.2467524 , -0.0269121 ,  0.11394349,\n",
       "       -0.26700324,  0.36797878, -0.33213878, -0.320999  , -0.03105091,\n",
       "       -0.01788925, -0.4120324 , -0.01183425,  0.0141205 , -0.41857877,\n",
       "       -0.04758538, -0.04544438, -0.17185563, -0.30771923, -0.00676738,\n",
       "        0.07450412,  0.14332   ,  0.06675501, -0.23973298,  0.17384434,\n",
       "       -0.10542063, -0.19367987, -0.09322474, -0.418527  ,  0.14046675,\n",
       "        0.34264448, -0.18632199,  0.0389975 ,  0.14402676, -0.04986487,\n",
       "       -0.11543176, -0.2545445 ,  0.14266202,  0.27471748,  0.0079375 ,\n",
       "       -0.14509262,  0.20125811,  0.22579625,  0.08263087, -0.03086439,\n",
       "        0.22810088, -0.14383201,  0.11217888,  0.12351375, -0.04662014,\n",
       "        0.03277338, -0.0668765 , -0.01429263, -0.02440687,  0.115247  ,\n",
       "       -0.06756738,  0.17035699,  0.13134988,  0.27908275, -0.39695752,\n",
       "        0.07546204,  0.12670626,  0.030109  ,  0.13146387,  0.08153588,\n",
       "       -0.35618636,  0.02789063,  0.09644613,  0.03024087,  0.11288763,\n",
       "       -0.0470105 ,  0.083297  , -0.19765326,  0.1939025 , -0.17408144,\n",
       "       -0.03410025, -0.29188365,  0.06836513, -0.11379087,  0.17760183,\n",
       "        0.0460275 , -0.07095987,  0.17035837,  0.00723087, -0.04813525,\n",
       "       -0.07019138,  0.12578554, -0.49508387, -0.0293907 ,  0.0912805 ,\n",
       "       -0.07047888, -0.14484507,  0.11000374,  0.23221269,  0.07575612,\n",
       "        0.2107177 , -0.00523663,  0.21724625, -0.04822462, -0.09642936,\n",
       "        0.00659313,  0.1067925 ,  0.11852536,  0.22509938,  0.13205758,\n",
       "       -0.09847125,  0.0346231 ,  0.0844025 , -0.053913  ,  0.12482549,\n",
       "       -0.2205879 , -0.06405988,  0.17144525,  0.131021  , -0.34052062,\n",
       "        1.0522763 ,  0.01132712,  0.15206775,  0.09423649,  0.11173812,\n",
       "       -0.13028413,  0.21974795,  0.08384912, -0.07657875, -0.3010299 ,\n",
       "       -0.32306057, -0.08781188,  0.16856986,  0.12006875, -0.01119399,\n",
       "        0.00752812,  0.186029  ,  0.10647699, -0.04026062,  0.03546663,\n",
       "        0.20003861, -0.01798458, -0.08460963, -0.01693788,  0.12155938,\n",
       "        0.06736262,  0.18779951, -0.03071299, -0.02902487, -0.0801667 ,\n",
       "        0.12087475,  0.01459587,  0.0770665 , -0.3766111 , -0.166125  ,\n",
       "        0.0504495 , -0.03433199,  0.25192988, -0.2101345 , -0.03260563,\n",
       "        0.01635125, -0.06013663,  0.19531424,  0.20191363, -0.47157475,\n",
       "       -0.0904045 , -0.09908313,  0.24699798, -0.04626013, -0.03931262,\n",
       "        0.21786726, -0.26702642, -0.15141988,  0.1316955 ,  0.39294776,\n",
       "        0.08776863,  0.0516995 , -0.04898775, -0.008698  , -0.11328526,\n",
       "       -0.01866299, -0.35834873, -0.05978112, -0.2117145 ,  0.07610384,\n",
       "        0.08024587, -0.11412679, -0.211824  , -0.0574548 ,  0.29985523,\n",
       "        0.09669325, -0.12103912,  0.16476138, -0.04289825, -0.0459525 ,\n",
       "        0.11577936, -1.8472499 , -0.11779313,  0.1293405 , -0.11447674,\n",
       "        0.03383375,  0.00932   ,  0.21137936, -0.01516025, -0.08616462,\n",
       "        0.24569151,  0.13278376,  0.2159009 , -0.16038813, -0.1453059 ,\n",
       "        0.06087281, -0.22556888, -0.00926037, -0.03358388,  0.17774624,\n",
       "       -0.40541375,  0.11658674, -0.24693875,  0.03953071,  0.15452224],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_vec(df['question1'][0][:-1], pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
