{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d0efd9-284f-4dc7-a8b7-0aa4c735ef2e",
   "metadata": {},
   "source": [
    "# Utils Alejandro Astruc Lopez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe60a52a-1797-4d1f-9a9f-b294910bf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from utils_Alejandro import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3575f2-8f33-41af-b940-13ada8d532a0",
   "metadata": {},
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "531f3ca3-c915-4be7-b45a-ceef26e21b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import spacy \n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from utils import *\n",
    "\n",
    "'''\n",
    "    These are the main functions to manipulate\n",
    "    and work around word2vect. They use regular\n",
    "    expressions, and a pretrained pipeline from\n",
    "    spacy to perform a series of operations to\n",
    "    the data before introducing it into the \n",
    "    word2vec model.\n",
    "'''\n",
    "\n",
    "def cleaning(doc):\n",
    "    '''\n",
    "        Cleaning routine that lemmatizaises\n",
    "        data.\n",
    "    '''\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "def preproces_train_data_Word2Vect(df, save=True):\n",
    "    '''\n",
    "        Preprocessing of the training data for the \n",
    "        word2vect data.\n",
    "    '''\n",
    "    #Drop nans\n",
    "    df.dropna(inplace=True)\n",
    "    #Build training data for Word2vect:\n",
    "    #First separate question marks and words using regular expressions\n",
    "    pattern = r\"(\\w+|[?!.])\"\n",
    "    sentences = list(np.append(df['question1'].values,df['question2'].values))\n",
    "    sentences = [' '.join(re.findall(pattern, sent)) for sent in sentences]\n",
    "    brief_cleaning = (re.sub(\"[^\\w'?]+\", ' ', str(row)).lower() for row in sentences)\n",
    "    \n",
    "    #Import spacy \n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    \n",
    "    #Use the spacy pipeline separating words etc\n",
    "    txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000,  n_process=-1)]\n",
    "    df_clean = pd.DataFrame({'clean': txt})\n",
    "    df_clean = df_clean.dropna().drop_duplicates()\n",
    "    if save:\n",
    "        df_clean.to_csv('clean_w2v_data.csv', index=False)\n",
    "    return df_clean\n",
    "\n",
    "def preproces_data_Word2Vect(df):\n",
    "    '''\n",
    "        Cleaning the data that will be the\n",
    "        input of the word2vect.\n",
    "    '''\n",
    "    string_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    out = []\n",
    "    for col in string_columns:\n",
    "        sentences = list(df[col].values)\n",
    "        #First separate question marks and words using regular expressions\n",
    "        pattern = r\"(\\w+|[?!.])\"\n",
    "        sentences = list(np.append(df['question1'].values,df['question2'].values))\n",
    "        sentences = [' '.join(re.findall(pattern, sent)) for sent in sentences]\n",
    "        brief_cleaning = (re.sub(\"[^\\w'?]+\", ' ', str(row)).lower() for row in sentences)\n",
    "        \n",
    "        #Import spacy \n",
    "        nlp = spacy.load('en_core_web_sm') \n",
    "        \n",
    "        #Use the spacy pipeline separating words etc\n",
    "        txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000,  n_process=-1)]\n",
    "        out.append(txt)\n",
    "    return np.array(out)\n",
    "\n",
    "def train_Word2Vect(df_clean, num_features = 300, num_epochs = 20,\n",
    "                    min_word_count = 0, num_workers = multiprocessing.cpu_count(), context_size = 5, \n",
    "                    downsampling = 1e-3, seed = 1, sg = 0, save=True):\n",
    "    '''\n",
    "        Function for training the word2vect from\n",
    "        already cleaned data.\n",
    "    '''\n",
    "    \n",
    "    word2vec = w2v.Word2Vec(\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    vector_size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    "    )\n",
    "    \n",
    "    word2vec.build_vocab(sent, progress_per=10000)\n",
    "    word2vec.train(sent, total_examples=word2vec.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "    if save:\n",
    "        word2vec.save(\"word2vec.model\")\n",
    "    return word2vec.wv\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    '''\n",
    "        Routine to separate sentence into\n",
    "        wordlist.\n",
    "    '''\n",
    "    clean = re.sub(\"[^a-zA-Z0-9]\", \" \", str(raw))\n",
    "    clean = clean.lower()\n",
    "    words = clean.split()\n",
    "    return words\n",
    "    \n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    '''\n",
    "        Routine to convert sentence into vector\n",
    "        given a word2vect keyevector.\n",
    "    '''\n",
    "    word_list = sentence_to_wordlist(sentence)\n",
    "    word_vectors = []\n",
    "    for w in word_list:\n",
    "        if word2vec.__contains__(w):\n",
    "            if word2vec.key_to_index[w] < len(word2vec.vectors):\n",
    "                word_vectors.append(word2vec[str(w)])\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def get_Word2Vect(df, word2vec=Word2Vec.load(\"word2vec.model\").wv, phrase2vec=doc_to_vec):\n",
    "    '''\n",
    "        Function that receives data to be cleaned and\n",
    "        then converts it to vectors.\n",
    "    '''\n",
    "    data = preproces_data_Word2Vect(df)\n",
    "    out = []\n",
    "    if len(data.shape) >1:\n",
    "        for val in data[:,0]:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = np.array(out)\n",
    "        for i in range(1, data.shape[1]):\n",
    "            out = []\n",
    "            for val in data[:,i]:\n",
    "                out.append(doc_to_vec(val,word2vec))\n",
    "            out = np.array(out)\n",
    "            array = np.stack((array,out))\n",
    "    else:\n",
    "        for val in data:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = np.array(out)\n",
    "    return array\n",
    "\n",
    "def get_Word2Vect_from_clean(df, word2vec=Word2Vec.load(\"word2vec.model\").wv, phrase2vec=doc_to_vec):\n",
    "    '''\n",
    "        Function that receives celan data and\n",
    "        then converts it to vectors.\n",
    "\n",
    "        A suggeste input to word2vec is:\n",
    "        word2vec = KeyedVectors.load(\"pretrained.model\")\n",
    "\n",
    "        \"pretrained.model\" is a keyevector from the \n",
    "        pretrained model: glove-wiki-gigaword-300\n",
    "        you can also download it as such:\n",
    "        \n",
    "        gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "        \n",
    "        I recommend saving with .save('file_path')\n",
    "        as it will be faster for future use:\n",
    "        \n",
    "        pre.save(\"pretrained.model\")\n",
    "    '''\n",
    "    data = df.values\n",
    "    out = []\n",
    "    if len(data.shape) >1:\n",
    "        for val in data[:,0]:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = out[:]\n",
    "        for i in range(1, data.shape[1]):\n",
    "            out = []\n",
    "            for val in data[:,i]:\n",
    "                out.append(doc_to_vec(val,word2vec))\n",
    "            array = [array,out]\n",
    "    else:\n",
    "        for val in data:\n",
    "            out.append(doc_to_vec(val,word2vec))\n",
    "        array = out[:]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b277c-d7a6-428d-bfec-6e8f5626a47d",
   "metadata": {},
   "source": [
    "# Training and tests of word2vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632bf7d-ee6c-46be-9530-509506d1262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "df = pd.read_csv(\"../nlp_deliv1_materials/quora_train_data.csv\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1998f105-0de5-420d-bc91-85781a384fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('clean_w2v_data.csv')\n",
    "word2vec = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0178a8f-38d3-4f09-a7c5-981ada4f5778",
   "metadata": {},
   "source": [
    "Here is an example using a pre trained word to vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f734da-ee74-4dd0-940d-f41221fa8821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18908401,  0.14575334, -0.24050768, ..., -0.21747333,\n",
       "         0.08525033,  0.17485468],\n",
       "       [ 0.1206454 ,  0.1098704 ,  0.171647  , ..., -0.09448619,\n",
       "        -0.08320801,  0.0506992 ],\n",
       "       [-0.42076406,  0.17165159,  0.25065342, ..., -0.0953576 ,\n",
       "         0.03478063,  0.1543766 ],\n",
       "       ...,\n",
       "       [-0.219928  , -0.029034  , -0.1162586 , ..., -0.072036  ,\n",
       "        -0.038287  ,  0.24115232],\n",
       "       [ 0.241744  ,  0.04891361,  0.0080844 , ...,  0.028048  ,\n",
       "        -0.2740474 ,  0.479464  ],\n",
       "       [-0.03195641, -0.10047483,  0.014281  , ..., -0.03649074,\n",
       "        -0.0140646 ,  0.21292908]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_Word2Vect_from_clean(df_clean, word2vec=KeyedVectors.load(\"pretrained.model\"), phrase2vec=doc_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff3a1720-9f90-4bf2-aef3-94ac3cf5d996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 415665, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_Word2Vect(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf23ee7b-30e1-4435-92fd-1b73d7ac9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "pre = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d4299cf-1191-4f76-941f-47eef8a69af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.save(\"pretrained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a77ae0ad-f101-4842-a406-2c57b4667773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20577545,  0.12746239, -0.04717724, -0.02534887,  0.04027225,\n",
       "        0.152412  ,  0.03551663, -0.03253975,  0.08493976, -1.6322538 ,\n",
       "        0.21031661, -0.1983688 , -0.11061212,  0.08063625, -0.05675174,\n",
       "       -0.07887324, -0.12003312, -0.15204847,  0.21590875,  0.13083738,\n",
       "        0.21441011,  0.40429187,  0.27155975, -0.01802613, -0.27786648,\n",
       "       -0.00815025,  0.26502863, -0.16491945,  0.08708687, -0.01988237,\n",
       "        0.06162221,  0.17012987, -0.3386976 , -0.19019699, -1.0490575 ,\n",
       "        0.12061688, -0.23097901, -0.086456  ,  0.00959194,  0.04805613,\n",
       "       -0.08873121, -0.27393562, -0.06129425, -0.12130875,  0.184645  ,\n",
       "        0.13793913,  0.21891037,  0.13277763,  0.00689214,  0.02593046,\n",
       "        0.10792162, -0.14590938,  0.00507363, -0.09312012, -0.23943488,\n",
       "        0.39449978,  0.07809716, -0.09254488,  0.07620325,  0.08252013,\n",
       "        0.4164734 , -0.090637  ,  0.07884928,  0.46675876, -0.14334024,\n",
       "       -0.24197625,  0.1956075 ,  0.09268922,  0.157938  , -0.13387226,\n",
       "        0.04115876, -0.04514462,  0.00606438,  0.45864874,  0.07632375,\n",
       "       -0.14317614,  0.00227937,  0.2105791 , -0.13032913, -0.15482712,\n",
       "       -0.09600829,  0.06206788,  0.3980264 , -0.14632386,  0.00395313,\n",
       "        0.11418659, -0.27903688,  0.2467524 , -0.0269121 ,  0.11394349,\n",
       "       -0.26700324,  0.36797878, -0.33213878, -0.320999  , -0.03105091,\n",
       "       -0.01788925, -0.4120324 , -0.01183425,  0.0141205 , -0.41857877,\n",
       "       -0.04758538, -0.04544438, -0.17185563, -0.30771923, -0.00676738,\n",
       "        0.07450412,  0.14332   ,  0.06675501, -0.23973298,  0.17384434,\n",
       "       -0.10542063, -0.19367987, -0.09322474, -0.418527  ,  0.14046675,\n",
       "        0.34264448, -0.18632199,  0.0389975 ,  0.14402676, -0.04986487,\n",
       "       -0.11543176, -0.2545445 ,  0.14266202,  0.27471748,  0.0079375 ,\n",
       "       -0.14509262,  0.20125811,  0.22579625,  0.08263087, -0.03086439,\n",
       "        0.22810088, -0.14383201,  0.11217888,  0.12351375, -0.04662014,\n",
       "        0.03277338, -0.0668765 , -0.01429263, -0.02440687,  0.115247  ,\n",
       "       -0.06756738,  0.17035699,  0.13134988,  0.27908275, -0.39695752,\n",
       "        0.07546204,  0.12670626,  0.030109  ,  0.13146387,  0.08153588,\n",
       "       -0.35618636,  0.02789063,  0.09644613,  0.03024087,  0.11288763,\n",
       "       -0.0470105 ,  0.083297  , -0.19765326,  0.1939025 , -0.17408144,\n",
       "       -0.03410025, -0.29188365,  0.06836513, -0.11379087,  0.17760183,\n",
       "        0.0460275 , -0.07095987,  0.17035837,  0.00723087, -0.04813525,\n",
       "       -0.07019138,  0.12578554, -0.49508387, -0.0293907 ,  0.0912805 ,\n",
       "       -0.07047888, -0.14484507,  0.11000374,  0.23221269,  0.07575612,\n",
       "        0.2107177 , -0.00523663,  0.21724625, -0.04822462, -0.09642936,\n",
       "        0.00659313,  0.1067925 ,  0.11852536,  0.22509938,  0.13205758,\n",
       "       -0.09847125,  0.0346231 ,  0.0844025 , -0.053913  ,  0.12482549,\n",
       "       -0.2205879 , -0.06405988,  0.17144525,  0.131021  , -0.34052062,\n",
       "        1.0522763 ,  0.01132712,  0.15206775,  0.09423649,  0.11173812,\n",
       "       -0.13028413,  0.21974795,  0.08384912, -0.07657875, -0.3010299 ,\n",
       "       -0.32306057, -0.08781188,  0.16856986,  0.12006875, -0.01119399,\n",
       "        0.00752812,  0.186029  ,  0.10647699, -0.04026062,  0.03546663,\n",
       "        0.20003861, -0.01798458, -0.08460963, -0.01693788,  0.12155938,\n",
       "        0.06736262,  0.18779951, -0.03071299, -0.02902487, -0.0801667 ,\n",
       "        0.12087475,  0.01459587,  0.0770665 , -0.3766111 , -0.166125  ,\n",
       "        0.0504495 , -0.03433199,  0.25192988, -0.2101345 , -0.03260563,\n",
       "        0.01635125, -0.06013663,  0.19531424,  0.20191363, -0.47157475,\n",
       "       -0.0904045 , -0.09908313,  0.24699798, -0.04626013, -0.03931262,\n",
       "        0.21786726, -0.26702642, -0.15141988,  0.1316955 ,  0.39294776,\n",
       "        0.08776863,  0.0516995 , -0.04898775, -0.008698  , -0.11328526,\n",
       "       -0.01866299, -0.35834873, -0.05978112, -0.2117145 ,  0.07610384,\n",
       "        0.08024587, -0.11412679, -0.211824  , -0.0574548 ,  0.29985523,\n",
       "        0.09669325, -0.12103912,  0.16476138, -0.04289825, -0.0459525 ,\n",
       "        0.11577936, -1.8472499 , -0.11779313,  0.1293405 , -0.11447674,\n",
       "        0.03383375,  0.00932   ,  0.21137936, -0.01516025, -0.08616462,\n",
       "        0.24569151,  0.13278376,  0.2159009 , -0.16038813, -0.1453059 ,\n",
       "        0.06087281, -0.22556888, -0.00926037, -0.03358388,  0.17774624,\n",
       "       -0.40541375,  0.11658674, -0.24693875,  0.03953071,  0.15452224],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_vec(df['question1'][0][:-1], pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bf383-f358-4bee-b2c5-1a724884357a",
   "metadata": {},
   "source": [
    "# Use in a logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3f6c0-2c01-42fa-bb2c-d921168c2ee8",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d43d1ce-d8fc-47d6-88a5-8be55fc33587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346692</td>\n",
       "      <td>38482</td>\n",
       "      <td>10706</td>\n",
       "      <td>Why do I get easily bored with everything?</td>\n",
       "      <td>Why do I get bored with things so quickly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>327668</td>\n",
       "      <td>454117</td>\n",
       "      <td>345117</td>\n",
       "      <td>How do I study for Honeywell company recruitment?</td>\n",
       "      <td>How do I study for Honeywell company recruitme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272993</td>\n",
       "      <td>391373</td>\n",
       "      <td>391374</td>\n",
       "      <td>Which search engine algorithm is Quora using?</td>\n",
       "      <td>Why is Quora not using reliable search engine?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54070</td>\n",
       "      <td>82673</td>\n",
       "      <td>95496</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Can someone who thinks about suicide for 7 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46450</td>\n",
       "      <td>38384</td>\n",
       "      <td>72436</td>\n",
       "      <td>How do I see who is viewing my Instagram videos?</td>\n",
       "      <td>Can one tell who viewed my Instagram videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323427</th>\n",
       "      <td>192476</td>\n",
       "      <td>292119</td>\n",
       "      <td>292120</td>\n",
       "      <td>Is it okay to use a laptop while it is chargin...</td>\n",
       "      <td>Is it OK to use your phone while charging?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323428</th>\n",
       "      <td>17730</td>\n",
       "      <td>33641</td>\n",
       "      <td>33642</td>\n",
       "      <td>How can dogs understand human language?</td>\n",
       "      <td>Can dogs understand the human language?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323429</th>\n",
       "      <td>28030</td>\n",
       "      <td>52012</td>\n",
       "      <td>52013</td>\n",
       "      <td>What's your favourite lotion?</td>\n",
       "      <td>What's your favourite skin lotion?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323430</th>\n",
       "      <td>277869</td>\n",
       "      <td>397054</td>\n",
       "      <td>120852</td>\n",
       "      <td>How does one become a hedge fund manager?</td>\n",
       "      <td>What should I do to become a hedge fund manager?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323431</th>\n",
       "      <td>249342</td>\n",
       "      <td>362958</td>\n",
       "      <td>362959</td>\n",
       "      <td>How did the US acquire over 80 trillion financ...</td>\n",
       "      <td>We've thought about evil geniuses ruling the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       346692   38482   10706   \n",
       "1       327668  454117  345117   \n",
       "2       272993  391373  391374   \n",
       "3        54070   82673   95496   \n",
       "4        46450   38384   72436   \n",
       "...        ...     ...     ...   \n",
       "323427  192476  292119  292120   \n",
       "323428   17730   33641   33642   \n",
       "323429   28030   52012   52013   \n",
       "323430  277869  397054  120852   \n",
       "323431  249342  362958  362959   \n",
       "\n",
       "                                                question1  \\\n",
       "0              Why do I get easily bored with everything?   \n",
       "1       How do I study for Honeywell company recruitment?   \n",
       "2           Which search engine algorithm is Quora using?   \n",
       "3                           How can I smartly cut myself?   \n",
       "4        How do I see who is viewing my Instagram videos?   \n",
       "...                                                   ...   \n",
       "323427  Is it okay to use a laptop while it is chargin...   \n",
       "323428            How can dogs understand human language?   \n",
       "323429                      What's your favourite lotion?   \n",
       "323430          How does one become a hedge fund manager?   \n",
       "323431  How did the US acquire over 80 trillion financ...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       Why do I get bored with things so quickly and ...             1  \n",
       "1       How do I study for Honeywell company recruitme...             1  \n",
       "2          Why is Quora not using reliable search engine?             0  \n",
       "3       Can someone who thinks about suicide for 7 yea...             0  \n",
       "4            Can one tell who viewed my Instagram videos?             1  \n",
       "...                                                   ...           ...  \n",
       "323427         Is it OK to use your phone while charging?             0  \n",
       "323428            Can dogs understand the human language?             0  \n",
       "323429                 What's your favourite skin lotion?             1  \n",
       "323430   What should I do to become a hedge fund manager?             1  \n",
       "323431  We've thought about evil geniuses ruling the w...             0  \n",
       "\n",
       "[323429 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data and drop nans\n",
    "df = pd.read_csv(\"../nlp_deliv1_materials/quora_train_data.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7632a0-2d99-459e-9bff-ba4feac543be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we clean a prepare the data for vectorization\n",
    "#We used the cleaning functions defined by Alba.\n",
    "q1 = preprocess_data(df['question1'])\n",
    "q2 = preprocess_data(df['question1'])\n",
    "df_clean = pd.DataFrame({'q1': q1, 'q2':q2})\n",
    "df_clean.to_csv('clean_w2v_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce35c1-7d0e-4eb1-b6aa-3ccb647b7766",
   "metadata": {},
   "source": [
    "## 2. Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e9e74f-6b5e-4c08-bfa5-a8407966bd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\anaconda3\\envs\\quora_challenge_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Alejandro\\anaconda3\\envs\\quora_challenge_env\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#We proceed to vetorize the entences with the functions we defined\n",
    "df_clean = pd.read_csv('clean_w2v_data.csv')\n",
    "vec = get_Word2Vect_from_clean(df_clean)# KeyedVectors.load(\"pretrained.model\"))\n",
    "df['q1'] = vec[0]\n",
    "df['q2'] = vec[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c71e75-f85e-4803-b834-38b4bda72fa2",
   "metadata": {},
   "source": [
    "Because of the limited word2vect we trained some sentences cannot be translated to a vector. Further on we will use a pretrained word to vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47863000-7fc7-48dd-b0c6-3cc12b4456ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346692</td>\n",
       "      <td>38482</td>\n",
       "      <td>10706</td>\n",
       "      <td>Why do I get easily bored with everything?</td>\n",
       "      <td>Why do I get bored with things so quickly and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.24570484, -0.30564713, -0.07970454, 0.1097...</td>\n",
       "      <td>[-0.24570484, -0.30564713, -0.07970454, 0.1097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>327668</td>\n",
       "      <td>454117</td>\n",
       "      <td>345117</td>\n",
       "      <td>How do I study for Honeywell company recruitment?</td>\n",
       "      <td>How do I study for Honeywell company recruitme...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.42143765, 0.11858634, -0.05358868, -0.2265...</td>\n",
       "      <td>[-0.42143765, 0.11858634, -0.05358868, -0.2265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272993</td>\n",
       "      <td>391373</td>\n",
       "      <td>391374</td>\n",
       "      <td>Which search engine algorithm is Quora using?</td>\n",
       "      <td>Why is Quora not using reliable search engine?</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.57006973, -0.58232933, -0.2590795, -1.1337...</td>\n",
       "      <td>[-0.57006973, -0.58232933, -0.2590795, -1.1337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54070</td>\n",
       "      <td>82673</td>\n",
       "      <td>95496</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Can someone who thinks about suicide for 7 yea...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.034957096, 0.56301516, 0.6061619, 0.91554,...</td>\n",
       "      <td>[-0.034957096, 0.56301516, 0.6061619, 0.91554,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46450</td>\n",
       "      <td>38384</td>\n",
       "      <td>72436</td>\n",
       "      <td>How do I see who is viewing my Instagram videos?</td>\n",
       "      <td>Can one tell who viewed my Instagram videos?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0910532, -0.39877188, -0.3537015, -0.256290...</td>\n",
       "      <td>[0.0910532, -0.39877188, -0.3537015, -0.256290...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323427</th>\n",
       "      <td>192476</td>\n",
       "      <td>292119</td>\n",
       "      <td>292120</td>\n",
       "      <td>Is it okay to use a laptop while it is chargin...</td>\n",
       "      <td>Is it OK to use your phone while charging?</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.20693651, -0.1512374, -0.0787127, -1.18186...</td>\n",
       "      <td>[-0.20693651, -0.1512374, -0.0787127, -1.18186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323428</th>\n",
       "      <td>17730</td>\n",
       "      <td>33641</td>\n",
       "      <td>33642</td>\n",
       "      <td>How can dogs understand human language?</td>\n",
       "      <td>Can dogs understand the human language?</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6491662, -0.32742617, -0.045254048, 0.11678...</td>\n",
       "      <td>[0.6491662, -0.32742617, -0.045254048, 0.11678...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323429</th>\n",
       "      <td>28030</td>\n",
       "      <td>52012</td>\n",
       "      <td>52013</td>\n",
       "      <td>What's your favourite lotion?</td>\n",
       "      <td>What's your favourite skin lotion?</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.74212116, -0.5065593, 0.42237905, 0.066984...</td>\n",
       "      <td>[-0.74212116, -0.5065593, 0.42237905, 0.066984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323430</th>\n",
       "      <td>277869</td>\n",
       "      <td>397054</td>\n",
       "      <td>120852</td>\n",
       "      <td>How does one become a hedge fund manager?</td>\n",
       "      <td>What should I do to become a hedge fund manager?</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0831171, -0.85846317, 0.5749842, -0.3321251...</td>\n",
       "      <td>[0.0831171, -0.85846317, 0.5749842, -0.3321251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323431</th>\n",
       "      <td>249342</td>\n",
       "      <td>362958</td>\n",
       "      <td>362959</td>\n",
       "      <td>How did the US acquire over 80 trillion financ...</td>\n",
       "      <td>We've thought about evil geniuses ruling the w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.49628413, 0.1497839, -0.19767533, 0.286448...</td>\n",
       "      <td>[-0.49628413, 0.1497839, -0.19767533, 0.286448...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322393 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       346692   38482   10706   \n",
       "1       327668  454117  345117   \n",
       "2       272993  391373  391374   \n",
       "3        54070   82673   95496   \n",
       "4        46450   38384   72436   \n",
       "...        ...     ...     ...   \n",
       "323427  192476  292119  292120   \n",
       "323428   17730   33641   33642   \n",
       "323429   28030   52012   52013   \n",
       "323430  277869  397054  120852   \n",
       "323431  249342  362958  362959   \n",
       "\n",
       "                                                question1  \\\n",
       "0              Why do I get easily bored with everything?   \n",
       "1       How do I study for Honeywell company recruitment?   \n",
       "2           Which search engine algorithm is Quora using?   \n",
       "3                           How can I smartly cut myself?   \n",
       "4        How do I see who is viewing my Instagram videos?   \n",
       "...                                                   ...   \n",
       "323427  Is it okay to use a laptop while it is chargin...   \n",
       "323428            How can dogs understand human language?   \n",
       "323429                      What's your favourite lotion?   \n",
       "323430          How does one become a hedge fund manager?   \n",
       "323431  How did the US acquire over 80 trillion financ...   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "0       Why do I get bored with things so quickly and ...             1   \n",
       "1       How do I study for Honeywell company recruitme...             1   \n",
       "2          Why is Quora not using reliable search engine?             0   \n",
       "3       Can someone who thinks about suicide for 7 yea...             0   \n",
       "4            Can one tell who viewed my Instagram videos?             1   \n",
       "...                                                   ...           ...   \n",
       "323427         Is it OK to use your phone while charging?             0   \n",
       "323428            Can dogs understand the human language?             0   \n",
       "323429                 What's your favourite skin lotion?             1   \n",
       "323430   What should I do to become a hedge fund manager?             1   \n",
       "323431  We've thought about evil geniuses ruling the w...             0   \n",
       "\n",
       "                                                       q1  \\\n",
       "0       [-0.24570484, -0.30564713, -0.07970454, 0.1097...   \n",
       "1       [-0.42143765, 0.11858634, -0.05358868, -0.2265...   \n",
       "2       [-0.57006973, -0.58232933, -0.2590795, -1.1337...   \n",
       "3       [-0.034957096, 0.56301516, 0.6061619, 0.91554,...   \n",
       "4       [0.0910532, -0.39877188, -0.3537015, -0.256290...   \n",
       "...                                                   ...   \n",
       "323427  [-0.20693651, -0.1512374, -0.0787127, -1.18186...   \n",
       "323428  [0.6491662, -0.32742617, -0.045254048, 0.11678...   \n",
       "323429  [-0.74212116, -0.5065593, 0.42237905, 0.066984...   \n",
       "323430  [0.0831171, -0.85846317, 0.5749842, -0.3321251...   \n",
       "323431  [-0.49628413, 0.1497839, -0.19767533, 0.286448...   \n",
       "\n",
       "                                                       q2  \n",
       "0       [-0.24570484, -0.30564713, -0.07970454, 0.1097...  \n",
       "1       [-0.42143765, 0.11858634, -0.05358868, -0.2265...  \n",
       "2       [-0.57006973, -0.58232933, -0.2590795, -1.1337...  \n",
       "3       [-0.034957096, 0.56301516, 0.6061619, 0.91554,...  \n",
       "4       [0.0910532, -0.39877188, -0.3537015, -0.256290...  \n",
       "...                                                   ...  \n",
       "323427  [-0.20693651, -0.1512374, -0.0787127, -1.18186...  \n",
       "323428  [0.6491662, -0.32742617, -0.045254048, 0.11678...  \n",
       "323429  [-0.74212116, -0.5065593, 0.42237905, 0.066984...  \n",
       "323430  [0.0831171, -0.85846317, 0.5749842, -0.3321251...  \n",
       "323431  [-0.49628413, 0.1497839, -0.19767533, 0.286448...  \n",
       "\n",
       "[322393 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7d587-15b0-4917-9a04-9d23644d8d15",
   "metadata": {},
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39c05aa5-9f5b-43ef-b65c-4152db1d059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.array([x for x in df['q1'].values]),np.array([x for x in df['q2'].values])))\n",
    "y = df['is_duplicate'].values\n",
    "\n",
    "X_, X_test, y_, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.05, random_state=123)\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_, y_, test_size=0.05, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995de70-24c3-4eb4-ab5a-97c0cb96569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic1 = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "006634d4-e20d-4a70-ad80-34705f2f27da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.674610512134012,\n",
       " 'roc_auc': 0.5922226268127543,\n",
       " 'precision': 0.6347794633928382,\n",
       " 'recall': 0.27792890147562976,\n",
       " 'f1': 0.3865934963036873}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics1 = evaluate_model(X_train, y_train, model=logistic1, display=False)\n",
    "train_metrics1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e054422-cf22-4b68-8c8c-faa8acb594c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6712158808933002,\n",
       " 'roc_auc': 0.5921966075376667,\n",
       " 'precision': 0.6304262807978099,\n",
       " 'recall': 0.28270782181690635,\n",
       " 'f1': 0.3903620293013682}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics1 = evaluate_model(X_val, y_val, model=logistic1, display=False)\n",
    "validation_metrics1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0345f30c-518c-47bb-99f6-65a74484635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.676302729528536,\n",
       " 'roc_auc': 0.5947708881807898,\n",
       " 'precision': 0.641113653699466,\n",
       " 'recall': 0.2821416582745888,\n",
       " 'f1': 0.39184149184149186}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics1  = evaluate_model(X_test, y_test, model=logistic1, display=False)\n",
    "test_metrics1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04162a-633c-4a90-a30d-30dbffcbe3a3",
   "metadata": {},
   "source": [
    "## 4. Train model with pretrained w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d852779-7274-4456-9cfa-4a52a5733351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "glove_vectors.save(\"pretrained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bfe46-8dc3-47fd-bbb9-ef1a31355d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('clean_w2v_data.csv')\n",
    "vec = get_Word2Vect_from_clean(df_clean, KeyedVectors.load(\"pretrained.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8dd8071-451e-4b97-859c-b704d81fc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../nlp_deliv1_materials/quora_train_data.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df['q1'] = vec[0]\n",
    "df['q2'] = vec[1]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8053567e-ef62-4a9a-938a-4a4c309f23d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((np.array([x for x in df['q1'].values]),np.array([x for x in df['q2'].values])))\n",
    "y = df['is_duplicate'].values\n",
    "\n",
    "X_, X_test, y_, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.05, random_state=123)\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_, y_, test_size=0.05, random_state=123)\n",
    "\n",
    "logistic2 = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3cccbd9-54eb-4f7d-8c6e-b3d2c0d47286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6865039124059583,\n",
       " 'roc_auc': 0.6213790993783148,\n",
       " 'precision': 0.6266878970948657,\n",
       " 'recall': 0.3726169039707438,\n",
       " 'f1': 0.4673539118843751}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics2 = evaluate_model(X_train, y_train, model=logistic2, display=False)\n",
    "train_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb656ae0-a8d4-4700-85eb-09113ddb7594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6890581266679685,\n",
       " 'roc_auc': 0.6197319858461495,\n",
       " 'precision': 0.6243499541144081,\n",
       " 'recall': 0.36511627906976746,\n",
       " 'f1': 0.4607743537645333}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics2 = evaluate_model(X_val, y_val, model=logistic2, display=False)\n",
    "validation_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd2b0c92-9577-44f1-8807-d4742cb71a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6874227059114519,\n",
       " 'roc_auc': 0.6215467097064837,\n",
       " 'precision': 0.6256352343308865,\n",
       " 'recall': 0.3727502102607233,\n",
       " 'f1': 0.46716559502477073}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics2  = evaluate_model(X_test, y_test, model=logistic2, display=False)\n",
    "test_metrics2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
