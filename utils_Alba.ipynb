{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5897bbdb",
   "metadata": {},
   "source": [
    "# Utils Alba Garcia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4195b2",
   "metadata": {},
   "source": [
    "Some utils for cleaning the data and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941c8a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement unidecodedata (from versions: none)\n",
      "ERROR: No matching distribution found for unidecodedata\n"
     ]
    }
   ],
   "source": [
    "pip install unidecodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a26c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import os\n",
    "import re\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "#import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d87e56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346692</td>\n",
       "      <td>38482</td>\n",
       "      <td>10706</td>\n",
       "      <td>Why do I get easily bored with everything?</td>\n",
       "      <td>Why do I get bored with things so quickly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>327668</td>\n",
       "      <td>454117</td>\n",
       "      <td>345117</td>\n",
       "      <td>How do I study for Honeywell company recruitment?</td>\n",
       "      <td>How do I study for Honeywell company recruitme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272993</td>\n",
       "      <td>391373</td>\n",
       "      <td>391374</td>\n",
       "      <td>Which search engine algorithm is Quora using?</td>\n",
       "      <td>Why is Quora not using reliable search engine?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54070</td>\n",
       "      <td>82673</td>\n",
       "      <td>95496</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Can someone who thinks about suicide for 7 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46450</td>\n",
       "      <td>38384</td>\n",
       "      <td>72436</td>\n",
       "      <td>How do I see who is viewing my Instagram videos?</td>\n",
       "      <td>Can one tell who viewed my Instagram videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323427</th>\n",
       "      <td>192476</td>\n",
       "      <td>292119</td>\n",
       "      <td>292120</td>\n",
       "      <td>Is it okay to use a laptop while it is chargin...</td>\n",
       "      <td>Is it OK to use your phone while charging?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323428</th>\n",
       "      <td>17730</td>\n",
       "      <td>33641</td>\n",
       "      <td>33642</td>\n",
       "      <td>How can dogs understand human language?</td>\n",
       "      <td>Can dogs understand the human language?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323429</th>\n",
       "      <td>28030</td>\n",
       "      <td>52012</td>\n",
       "      <td>52013</td>\n",
       "      <td>What's your favourite lotion?</td>\n",
       "      <td>What's your favourite skin lotion?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323430</th>\n",
       "      <td>277869</td>\n",
       "      <td>397054</td>\n",
       "      <td>120852</td>\n",
       "      <td>How does one become a hedge fund manager?</td>\n",
       "      <td>What should I do to become a hedge fund manager?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323431</th>\n",
       "      <td>249342</td>\n",
       "      <td>362958</td>\n",
       "      <td>362959</td>\n",
       "      <td>How did the US acquire over 80 trillion financ...</td>\n",
       "      <td>We've thought about evil geniuses ruling the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323432 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       346692   38482   10706   \n",
       "1       327668  454117  345117   \n",
       "2       272993  391373  391374   \n",
       "3        54070   82673   95496   \n",
       "4        46450   38384   72436   \n",
       "...        ...     ...     ...   \n",
       "323427  192476  292119  292120   \n",
       "323428   17730   33641   33642   \n",
       "323429   28030   52012   52013   \n",
       "323430  277869  397054  120852   \n",
       "323431  249342  362958  362959   \n",
       "\n",
       "                                                question1  \\\n",
       "0              Why do I get easily bored with everything?   \n",
       "1       How do I study for Honeywell company recruitment?   \n",
       "2           Which search engine algorithm is Quora using?   \n",
       "3                           How can I smartly cut myself?   \n",
       "4        How do I see who is viewing my Instagram videos?   \n",
       "...                                                   ...   \n",
       "323427  Is it okay to use a laptop while it is chargin...   \n",
       "323428            How can dogs understand human language?   \n",
       "323429                      What's your favourite lotion?   \n",
       "323430          How does one become a hedge fund manager?   \n",
       "323431  How did the US acquire over 80 trillion financ...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       Why do I get bored with things so quickly and ...             1  \n",
       "1       How do I study for Honeywell company recruitme...             1  \n",
       "2          Why is Quora not using reliable search engine?             0  \n",
       "3       Can someone who thinks about suicide for 7 yea...             0  \n",
       "4            Can one tell who viewed my Instagram videos?             1  \n",
       "...                                                   ...           ...  \n",
       "323427         Is it OK to use your phone while charging?             0  \n",
       "323428            Can dogs understand the human language?             0  \n",
       "323429                 What's your favourite skin lotion?             1  \n",
       "323430   What should I do to become a hedge fund manager?             1  \n",
       "323431  We've thought about evil geniuses ruling the w...             0  \n",
       "\n",
       "[323432 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./quora_train_data.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ceddfe",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b694ec1",
   "metadata": {},
   "source": [
    "Next, we define some functions using **regex** with the goal of preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "967cd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all lower case sentence\n",
    "def lowercase_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence to be put in lower case.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens extracted from the input sentence.\n",
    "    \"\"\"\n",
    "    new_sentence = sentence.lower()\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove punctuation from a sentence\n",
    "def remove_punctuation(sentence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence to remove punctuation.\n",
    "\n",
    "    Returns:\n",
    "    list: The sentence without punctuation symbols.\n",
    "    \"\"\"    \n",
    "    new_sentence = re.sub(r'[^\\w\\s]', '', sentence) # matches non words and non spaces (includes '?') \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove accents\n",
    "def remove_accents(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to remove accent\n",
    "    Return:\n",
    "      str : The sentence without accents\n",
    "    '''\n",
    "    new_sentence = unidecode.unidecode(sentence) \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove non-alpha characters and non-alphanumeric characters (that is, special characters: punctuation marks, spaces, accents)\n",
    "def remove_special_characters(sentence, numeric = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence to remove non-alphanumeric characters.\n",
    "    numeric (bool): if true, numbers are also removed\n",
    "\n",
    "    Returns:\n",
    "    str: The sentence without non-alphanumric characters (includes punctuation symbols and spaces).\n",
    "    \"\"\"\n",
    "    if numeric:\n",
    "        new_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence) # matches non-alpha characters \n",
    "    else:\n",
    "        new_sentence = re.sub(r'[^a-zA-Z0-9]', ' ', sentence) # matches non-alphanumeric characters\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove stop words\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence from which stop words will be removed.\n",
    "\n",
    "    Returns:\n",
    "    str: The input sentence with stop words removed.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english')) # predefined stop words in English\n",
    "    \n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    new_sentence = ' '.join(filtered_words)\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# Normalize spaces - Replace all consecutive whitespace characters in the text string with a single space.\n",
    "def normalize_spaces(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to normalize\n",
    "    Returns:\n",
    "      str: The final sentence normalized \n",
    "    '''\n",
    "    new_sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return new_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863b7ff",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f66bb",
   "metadata": {},
   "source": [
    "For tokenizing we just us **nltk.word_tokenize**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16f35e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From => This is an example sentence to test the given tokenizer. -> ['This', 'is', 'an', 'example', 'sentence', 'to', 'test', 'the', 'given', 'tokenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "sentence = 'This is an example sentence to test the given tokenizer.'\n",
    "print(f\"From => {sentence} -> {nltk.word_tokenize(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08831482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "\t Hello, hÃ¶w are you doing? I hope everything is \\ going well! LÃ©t's meet at 3:00 PM. (It's raining outside.)\n",
      "\n",
      "In lower case:\n",
      "\t hello, hÃ¶w are you doing? i hope everything is \\ going well! lÃ©t's meet at 3:00 pm. (it's raining outside.)\n",
      "\n",
      "Without punctuation symbols:\n",
      "\t Hello hÃ¶w are you doing I hope everything is  going well LÃ©ts meet at 300 PM Its raining outside\n",
      "\n",
      "Witout accents:\n",
      "\t Hello, how are you doing? I hope everything is \\ going well! Let's meet at 3:00 PM. (It's raining outside.)\n",
      "\n",
      "Without special characters:\n",
      "\t Hello  h w are you doing  I hope everything is   going well  L t s meet at 3 00 PM   It s raining outside  \n",
      "\n",
      "Normalized spaces after removing special characters:\n",
      "\t Hello h w are you doing I hope everything is going well L t s meet at 3 00 PM It s raining outside \n",
      "\n",
      "Without stop words:\n",
      "\t Hello , hÃ¶w ? hope everything \\ going well ! LÃ©t 's meet 3:00 PM . ( 's raining outside . )\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, hÃ¶w are you doing? I hope everything is \\ going well! LÃ©t's meet at 3:00 PM. (It's raining outside.)\"\n",
    "print(\"Original sentence:\\n\\t\", sentence)\n",
    "\n",
    "# All lowercase\n",
    "new_sentence = lowercase_sentence(sentence)\n",
    "print(\"\\nIn lower case:\\n\\t\", new_sentence)\n",
    "\n",
    "# Remove punctuation\n",
    "new_sentence = remove_punctuation(sentence)\n",
    "print(\"\\nWithout punctuation symbols:\\n\\t\", new_sentence)\n",
    "\n",
    "# Remove accents\n",
    "new_sentence = remove_accents(sentence)\n",
    "print(\"\\nWitout accents:\\n\\t\", new_sentence)\n",
    "\n",
    "# Remove special characters\n",
    "new_sentence = remove_special_characters(sentence)\n",
    "print(\"\\nWithout special characters:\\n\\t\", new_sentence)\n",
    "\n",
    "# Normalize spaces\n",
    "norm_sentence = normalize_spaces(new_sentence)\n",
    "print(\"\\nNormalized spaces after removing special characters:\\n\\t\", norm_sentence)\n",
    "\n",
    "# Remove stop words\n",
    "new_sentence = remove_stopwords(sentence)\n",
    "print(\"\\nWithout stop words:\\n\\t\", new_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0fb41",
   "metadata": {},
   "source": [
    "## Text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cec21",
   "metadata": {},
   "source": [
    "### Stemming and lemmantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0a9f4",
   "metadata": {},
   "source": [
    "Extracting basic text features: **stemming** and **lemmantization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22cee0",
   "metadata": {},
   "source": [
    "**To choose between stemmers**: the choice between PorterStemmer and LancasterStemmer depends on your specific requirements and the characteristics of your text data. If you need a more conservative approach with stems closer to the original words, PorterStemmer may be a better choice. However, if you prefer a more aggressive stemming approach that produces shorter stems, LancasterStemmer might be more suitable. It's often a good idea to experiment with both stemmers on your data to determine which one performs better for your particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10689aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "108a1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming (using both methods) -> remove prefixes and suffixes, may return non existing word \n",
    "def stem(sentence, type_porter = True):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence for stemming\n",
    "      type_porter (bool): if True we use the Porter method, if false, the Lancaster method\n",
    "    Returns:\n",
    "      str: The final sentence stemmed\n",
    "    '''\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    if type_porter:\n",
    "        for word in token_words:\n",
    "            sentence_stemmed.append(porter.stem(word))\n",
    "            sentence_stemmed.append(\" \")\n",
    "    else:\n",
    "        for word in token_words:\n",
    "            sentence_stemmed.append(lancaster.stem(word))\n",
    "            sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)\n",
    "\n",
    "\n",
    "# lemmantization (using wordnet_lemmatizer.lemmatize(w)) -> remove endings to return base word (it is a valid word)\n",
    "def lemma(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence for lemmantization\n",
    "      str: The final sentence lemmantized\n",
    "    '''\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_lemma = []\n",
    "    for word in token_words:\n",
    "        sentence_lemma.append(wordnet_lemmatizer.lemmatize(word)) # focus on verbs\n",
    "        sentence_lemma.append(\" \")\n",
    "    return \"\".join(sentence_lemma)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c44dd",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47561e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "\t Hello, hÃ¶w are you doing? I hope everything is \\ going well! LÃ©t's meet at 3:00 PM. (It's raining outside.)\n",
      "\n",
      "Stemmed sentence:\n",
      "\t hello , hÃ¶w are you do ? i hope everyth is \\ go well ! lÃ©t 's meet at 3:00 pm . ( it 's rain outsid . ) \n",
      "\n",
      "Lemmantization sentence:\n",
      "\t Hello , hÃ¶w are you doing ? I hope everything is \\ going well ! LÃ©t 's meet at 3:00 PM . ( It 's raining outside . ) \n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, hÃ¶w are you doing? I hope everything is \\ going well! LÃ©t's meet at 3:00 PM. (It's raining outside.)\"\n",
    "print(\"Original sentence:\\n\\t\", sentence)\n",
    "\n",
    "# stemming\n",
    "new_sentence = stem(sentence)\n",
    "print(\"\\nStemmed sentence:\\n\\t\", new_sentence)\n",
    "\n",
    "# lemmantization\n",
    "new_sentence = lemma(sentence)\n",
    "print(\"\\nLemmantization sentence:\\n\\t\", new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8006d8",
   "metadata": {},
   "source": [
    "**Observation:** nothing much changes with lemmantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3554d17",
   "metadata": {},
   "source": [
    "### Other text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6d02a",
   "metadata": {},
   "source": [
    "Extracting **other interesting text features** like the number of words, the number of common words between two sentences, if the first word is the same, if the last word is the same and the number of words that are in the same position between two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b88b6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in a sentence\n",
    "def number_words(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to count the number of words\n",
    "      \n",
    "    Returns:\n",
    "      int : The number of words in the given text\n",
    "    '''\n",
    "    return len(word_tokenize(sentence))\n",
    "\n",
    "\n",
    "# Number of common words between two sentences\n",
    "def number_common_words(s1, s2):\n",
    "    '''\n",
    "    Args:\n",
    "      s1 (str): First sentence\n",
    "      s2 (str): Second sentence\n",
    "    \n",
    "    Return:\n",
    "      int: The number of common words that the two sentences have in common\n",
    "    '''\n",
    "    # Tokenize\n",
    "    tokens1 = set(word_tokenize(s1))\n",
    "    tokens2 = set(word_tokenize(s2))\n",
    "    \n",
    "    common = tokens1 & tokens2 # list of common tokens\n",
    "    return len(common)\n",
    "\n",
    "\n",
    "# Number of common words in the same position\n",
    "def number_common_words_2(s1, s2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s1 (str): The first input sentence.\n",
    "      s2 (str): The second input sentence.\n",
    "\n",
    "    Returns:\n",
    "      int: The number of common words at the same position in both sentences.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(s1)\n",
    "    tokens2 = word_tokenize(s2)\n",
    "\n",
    "    min_length = min(len(tokens1), len(tokens2))\n",
    "\n",
    "    # Common words at the same position\n",
    "    common_count = 0\n",
    "    for i in range(min_length):\n",
    "        if tokens1[i].lower() == tokens2[i].lower():\n",
    "            common_count += 1\n",
    "\n",
    "    return common_count\n",
    "\n",
    "\n",
    "# If the first word of two sentences is equal\n",
    "def first_word_equal(s1, s2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s1 (str): First sentence\n",
    "      s2 (str): Second sentence\n",
    "    Returns:\n",
    "      A binary value indicating whether the firsts words of the two questions are equal.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(s1)\n",
    "    tokens2 = word_tokenize(s2)\n",
    "    \n",
    "    if tokens1[0].lower() == tokens2[0].lower():\n",
    "            return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# If the last word of two sentences is equal\n",
    "def last_word_equal(s1, s2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s1 (str): First sentence\n",
    "      s2 (str): Second sentence\n",
    "    Returns:\n",
    "      A binary value indicating whether the lasts words of the two questions are equal.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(s1)\n",
    "    tokens2 = word_tokenize(s2) # with word_tokenize, counts '.' as different token\n",
    "    \n",
    "    if tokens1[-1].lower() == tokens2[-1].lower():\n",
    "            return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b585e",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1b01f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence 1:\n",
      "\t This is an example sentence to test the count of words\n",
      "Original sentence 2:\n",
      "\t This is a second example to test the count of common words\n",
      "Original sentence 3:\n",
      "\t Another different sentence\n",
      "\n",
      "Number of words of sentence 1: 11\n",
      "\n",
      "Number of words of sentence 2: 12\n",
      "\n",
      "Number of common words between sentence 1 and sentence 2: 9\n",
      "\n",
      "Number of common words between sentence 1 and sentence 3: 1\n",
      "\n",
      "Number of common words in the same position between sentence 1 and sentence 2: 7\n",
      "\n",
      "Number of common words in the same position between sentence 1 and sentence 3: 0\n",
      "\n",
      "Comparing the first words of sentence1 and sentence2: 1\n",
      "\n",
      "Comparing the first words of sentence1 and sentence3: 0\n",
      "\n",
      "Comparing the last words of sentence1 and sentence2: 1\n",
      "\n",
      "Comparing the last words of sentence1 and sentence3: 0\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'This is an example sentence to test the count of words'\n",
    "sentence2 = 'This is a second example to test the count of common words'\n",
    "sentence3 = 'Another different sentence'\n",
    "print(\"Original sentence 1:\\n\\t\", sentence1)\n",
    "print(\"Original sentence 2:\\n\\t\", sentence2)\n",
    "print(\"Original sentence 3:\\n\\t\", sentence3)\n",
    "\n",
    "# number of words\n",
    "k1 = number_words(sentence1)\n",
    "k2 = number_words(sentence2)\n",
    "print(\"\\nNumber of words of sentence 1:\", k1)\n",
    "print(\"\\nNumber of words of sentence 2:\", k2)\n",
    "\n",
    "# number of common words\n",
    "k3 = number_common_words(sentence1, sentence2)\n",
    "print(\"\\nNumber of common words between sentence 1 and sentence 2:\", k3)\n",
    "\n",
    "k4 = number_common_words(sentence1, sentence3)\n",
    "print(\"\\nNumber of common words between sentence 1 and sentence 3:\", k4)\n",
    "\n",
    "# number of common words in the same postion\n",
    "k3 = number_common_words_2(sentence1, sentence2)\n",
    "print(\"\\nNumber of common words in the same position between sentence 1 and sentence 2:\", k3)\n",
    "\n",
    "k4 = number_common_words_2(sentence1, sentence3)\n",
    "print(\"\\nNumber of common words in the same position between sentence 1 and sentence 3:\", k4)\n",
    "\n",
    "# first and last words equal\n",
    "print(\"\\nComparing the first words of sentence1 and sentence2:\", first_word_equal(sentence1,sentence2))\n",
    "print(\"\\nComparing the first words of sentence1 and sentence3:\", first_word_equal(sentence1,sentence3))\n",
    "\n",
    "print(\"\\nComparing the last words of sentence1 and sentence2:\", last_word_equal(sentence1,sentence2))\n",
    "print(\"\\nComparing the last words of sentence1 and sentence3:\", last_word_equal(sentence1,sentence3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
