{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5897bbdb",
   "metadata": {},
   "source": [
    "# Utils Alba Garcia Romo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4195b2",
   "metadata": {},
   "source": [
    "Some utils for cleaning the data and feature extraction. Also, testing different models with those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8a26c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import os\n",
    "import re\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "#import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ceddfe",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d6c3d",
   "metadata": {},
   "source": [
    "### Some functions using regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b694ec1",
   "metadata": {},
   "source": [
    "Next, we define some functions using **regex** with the goal of preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "967cd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all lower case sentence\n",
    "def lowercase_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence to be put in lower case.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens extracted from the input sentence.\n",
    "    \"\"\"\n",
    "    new_sentence = sentence.lower()\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove punctuation from a sentence\n",
    "def remove_punctuation(sentence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence to remove punctuation.\n",
    "\n",
    "    Returns:\n",
    "    list: The sentence without punctuation symbols.\n",
    "    \"\"\"    \n",
    "    new_sentence = re.sub(r'[^\\w\\s]', '', sentence) # matches non words and non spaces (includes '?') \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove accents\n",
    "def remove_accents(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to remove accent\n",
    "    Return:\n",
    "      str : The sentence without accents\n",
    "    '''\n",
    "    new_sentence = unidecode.unidecode(sentence) \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove non-alpha characters and non-alphanumeric characters (that is, special characters: punctuation marks, spaces, accents)\n",
    "def remove_special_characters(sentence, numeric = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence to remove non-alphanumeric characters.\n",
    "    numeric (bool): if true, numbers are also removed\n",
    "\n",
    "    Returns:\n",
    "    str: The sentence without non-alphanumric characters (includes punctuation symbols and spaces).\n",
    "    \"\"\"\n",
    "    if numeric:\n",
    "        new_sentence = re.sub(r'[^a-zA-Z]', ' ', sentence) # matches non-alpha characters \n",
    "    else:\n",
    "        new_sentence = re.sub(r'[^a-zA-Z0-9]', ' ', sentence) # matches non-alphanumeric characters\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# remove stop words\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    sentence (str): The input sentence from which stop words will be removed.\n",
    "\n",
    "    Returns:\n",
    "    str: The input sentence with stop words removed.\n",
    "    \"\"\"\n",
    "    #stop_words = set(stopwords.words('english')) # predefined stop words in English\n",
    "    stop_words = set(['the', 'and', 'to', 'in', 'of', 'that', 'is', 'it', 'for',\n",
    "    'on', 'this', 'you', 'be', 'are', 'or', 'from', 'at', 'by', 'we',\n",
    "    'an', 'not', 'have', 'has', 'but', 'as', 'if', 'so', 'they', 'their',\n",
    "    'was', 'were','some', 'there', 'these', 'those', 'than', 'then', 'been', 'also',\n",
    "    'much', 'many', 'other']) # custom defined set\n",
    "    \n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    new_sentence = ' '.join(filtered_words)\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# Normalize spaces - Replace all consecutive whitespace characters in the text string with a single space.\n",
    "def normalize_spaces(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to normalize\n",
    "    Returns:\n",
    "      str: The final sentence normalized \n",
    "    '''\n",
    "    new_sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return new_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863b7ff",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f66bb",
   "metadata": {},
   "source": [
    "For tokenizing we just us **nltk.word_tokenize**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16f35e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From => This is an example sentence to test the given tokenizer. -> ['This', 'is', 'an', 'example', 'sentence', 'to', 'test', 'the', 'given', 'tokenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "sentence = 'This is an example sentence to test the given tokenizer.'\n",
    "print(f\"From => {sentence} -> {nltk.word_tokenize(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08831482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "\t Hello, höw are you doing? I hope everything is \\ going well! Lét's meet at 3:00 PM. (It's raining outside.)\n",
      "\n",
      "In lower case:\n",
      "\t hello, höw are you doing? i hope everything is \\ going well! lét's meet at 3:00 pm. (it's raining outside.)\n",
      "\n",
      "Without punctuation symbols:\n",
      "\t Hello höw are you doing I hope everything is  going well Léts meet at 300 PM Its raining outside\n",
      "\n",
      "Witout accents:\n",
      "\t Hello, how are you doing? I hope everything is \\ going well! Let's meet at 3:00 PM. (It's raining outside.)\n",
      "\n",
      "Without special characters:\n",
      "\t Hello  h w are you doing  I hope everything is   going well  L t s meet at 3 00 PM   It s raining outside  \n",
      "\n",
      "Without special characters nor numbers:\n",
      "\t Hello  h w are you doing  I hope everything is   going well  L t s meet at      PM   It s raining outside  \n",
      "\n",
      "Normalized spaces after removing special characters:\n",
      "\t Hello h w are you doing I hope everything is going well L t s meet at PM It s raining outside \n",
      "\n",
      "Without stop words:\n",
      "\t Hello , höw doing ? I hope everything \\ going well ! Lét 's meet 3:00 PM . ( 's raining outside . )\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, höw are you doing? I hope everything is \\ going well! Lét's meet at 3:00 PM. (It's raining outside.)\"\n",
    "print(\"Original sentence:\\n\\t\", sentence)\n",
    "\n",
    "# All lowercase\n",
    "new_sentence = lowercase_sentence(sentence)\n",
    "print(\"\\nIn lower case:\\n\\t\", new_sentence)\n",
    "\n",
    "# Remove punctuation\n",
    "new_sentence = remove_punctuation(sentence)\n",
    "print(\"\\nWithout punctuation symbols:\\n\\t\", new_sentence)\n",
    "\n",
    "# Remove accents\n",
    "new_sentence = remove_accents(sentence)\n",
    "print(\"\\nWitout accents:\\n\\t\", new_sentence)\n",
    "\n",
    "# Remove special characters\n",
    "new_sentence = remove_special_characters(sentence)\n",
    "print(\"\\nWithout special characters:\\n\\t\", new_sentence)\n",
    "new_sentence = remove_special_characters(sentence, numeric = True)\n",
    "print(\"\\nWithout special characters nor numbers:\\n\\t\", new_sentence)\n",
    "\n",
    "# Normalize spaces\n",
    "norm_sentence = normalize_spaces(new_sentence)\n",
    "print(\"\\nNormalized spaces after removing special characters:\\n\\t\", norm_sentence)\n",
    "\n",
    "# Remove stop words\n",
    "new_sentence = remove_stopwords(sentence)\n",
    "print(\"\\nWithout stop words:\\n\\t\", new_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4121e",
   "metadata": {},
   "source": [
    "### BK Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659631f5",
   "metadata": {},
   "source": [
    "Implemt a BK Tree to later perform **spelling correction** as part of the preprocess stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac076ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_nodes = []\n",
    "class BKTree:\n",
    "    def __init__(self, distfn, words):\n",
    "        self.distfn = distfn\n",
    "\n",
    "        it = iter(words)\n",
    "        root = next(it)\n",
    "        self.tree = (root, {})\n",
    "\n",
    "        for i in it:\n",
    "            self._add_word(self.tree, i)\n",
    "\n",
    "    def _add_word(self, parent, word):\n",
    "        pword, children = parent\n",
    "        d = self.distfn(word, pword)\n",
    "        if d in children:\n",
    "            self._add_word(children[d], word)\n",
    "        else:\n",
    "            children[d] = (word, {})\n",
    "            \n",
    "    def _search_descendants(self, parent, max_distance, distance, query_word):\n",
    "        node_word, children_dict = parent\n",
    "        dist_to_node = distance(query_word, node_word)\n",
    "        self.visited_nodes.append(node_word)\n",
    "        results = []\n",
    "\n",
    "        if dist_to_node <= max_distance:\n",
    "            results.append((dist_to_node, node_word))\n",
    "\n",
    "        I = range(max(0, dist_to_node - max_distance), dist_to_node + max_distance + 1)\n",
    "        for dist in I:\n",
    "            if dist in children_dict:\n",
    "                child = children_dict[dist]\n",
    "                if child[0] not in self.visited_nodes:\n",
    "                    results.extend(self._search_descendants(child, max_distance, distance, query_word))\n",
    "        return results\n",
    "\n",
    "    def query(self, query_word, max_distance):\n",
    "        self.visited_nodes = []\n",
    "        results = self._search_descendants(self.tree, max_distance, self.distfn, query_word)\n",
    "        sorted_results = sorted(results)\n",
    "        return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3283c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "# spellinc correction\n",
    "\n",
    "def correct_bktree(sentence, words):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to be corrected\n",
    "      words (set): The vocabulary that contains all known words\n",
    "    Returns:\n",
    "      str: The final sentence corrected \n",
    "    '''\n",
    "    bk_tree = BKTree(editdistance.eval, words)\n",
    "    correction = []\n",
    "    for w in sentence.split(\" \"):\n",
    "        if w in words:\n",
    "            correction.append(w)\n",
    "        else:\n",
    "            w_similar = bk_tree.query(w,2)\n",
    "            if len(w_similar)>0:\n",
    "                w_corrected = w_similar[0][1]\n",
    "                correction.append(w_corrected)\n",
    "            else:\n",
    "                # no word found, simply append the unedited word\n",
    "                correction.append(w)\n",
    "    #return correction\n",
    "    return ' '.join(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d6df9",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c59fa6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vocabulary we will use\n",
    "words = nltk.corpus.words.words() \n",
    "\n",
    "# sentence to correct\n",
    "phrase = \"the man wentt to the antimonarchik protest because he did not like the king\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "433f695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " the man wentt to the antimonarchik protest because he did not like the king\n",
      "Corrected:\n",
      " the man went to the antimonarchic protest because he did not like the king\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\\n\", phrase)\n",
    "print(\"Corrected:\\n\", correct_bktree(phrase, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cec21",
   "metadata": {},
   "source": [
    "### Stemming and lemmantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0a9f4",
   "metadata": {},
   "source": [
    "Another type of preprocessing: **stemming** and **lemmantization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22cee0",
   "metadata": {},
   "source": [
    "**To choose between stemmers**: the choice between PorterStemmer and LancasterStemmer depends on your specific requirements and the characteristics of your text data. If you need a more conservative approach with stems closer to the original words, PorterStemmer may be a better choice. However, if you prefer a more aggressive stemming approach that produces shorter stems, LancasterStemmer might be more suitable. It's often a good idea to experiment with both stemmers on your data to determine which one performs better for your particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "10689aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "108a1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming (using both methods) -> remove prefixes and suffixes, may return non existing word \n",
    "def stem(sentence, type_porter = True):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence for stemming\n",
    "      type_porter (bool): if True we use the Porter method, if false, the Lancaster method\n",
    "    Returns:\n",
    "      str: The final sentence stemmed\n",
    "    '''\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    if type_porter:\n",
    "        for word in token_words:\n",
    "            sentence_stemmed.append(porter.stem(word))\n",
    "            sentence_stemmed.append(\" \")\n",
    "    else:\n",
    "        for word in token_words:\n",
    "            sentence_stemmed.append(lancaster.stem(word))\n",
    "            sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)\n",
    "\n",
    "\n",
    "# lemmantization (using wordnet_lemmatizer.lemmatize(w)) -> remove endings to return base word (it is a valid word)\n",
    "def lemma(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence for lemmantization\n",
    "      str: The final sentence lemmantized\n",
    "    '''\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_lemma = []\n",
    "    for word in token_words:\n",
    "        sentence_lemma.append(wordnet_lemmatizer.lemmatize(word)) # focus on verbs\n",
    "        sentence_lemma.append(\" \")\n",
    "    return \"\".join(sentence_lemma)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c44dd",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47561e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "\t Hello, höw are you doing? I hope everything is \\ going well! Lét's meet at 3:00 PM. (It's raining outside.)\n",
      "\n",
      "Stemmed sentence (Porter):\n",
      "\t hello , höw are you do ? i hope everyth is \\ go well ! lét 's meet at 3:00 pm . ( it 's rain outsid . ) \n",
      "\n",
      "Stemmed sentence (Lancaster):\n",
      "\t hello , höw ar you doing ? i hop everyth is \\ going wel ! lét 's meet at 3:00 pm . ( it 's rain outsid . ) \n",
      "\n",
      "Lemmantization sentence:\n",
      "\t Hello , höw are you doing ? I hope everything is \\ going well ! Lét 's meet at 3:00 PM . ( It 's raining outside . ) \n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, höw are you doing? I hope everything is \\ going well! Lét's meet at 3:00 PM. (It's raining outside.)\"\n",
    "print(\"Original sentence:\\n\\t\", sentence)\n",
    "\n",
    "# stemming\n",
    "new_sentence = stem(sentence)\n",
    "print(\"\\nStemmed sentence (Porter):\\n\\t\", new_sentence)\n",
    "new_sentence = stem(sentence, type_porter = False)\n",
    "print(\"\\nStemmed sentence (Lancaster):\\n\\t\", new_sentence)\n",
    "\n",
    "# lemmantization\n",
    "new_sentence = lemma(sentence)\n",
    "print(\"\\nLemmantization sentence:\\n\\t\", new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8006d8",
   "metadata": {},
   "source": [
    "**Observation:** nothing much changes with lemmantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3554d17",
   "metadata": {},
   "source": [
    "## Text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6d02a",
   "metadata": {},
   "source": [
    "Extracting **other interesting text features** like the number of words, the number of common words between two sentences, if the first word is the same, if the last word is the same and the number of words that are in the same position between two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b88b6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in a sentence\n",
    "def number_words(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "      sentence (str): The input sentence to count the number of words\n",
    "      \n",
    "    Returns:\n",
    "      int : The number of words in the given text\n",
    "    '''\n",
    "    return len(word_tokenize(sentence))\n",
    "\n",
    "\n",
    "# Number of common words between two sentences\n",
    "def number_common_words(s1, s2):\n",
    "    '''\n",
    "    Args:\n",
    "      s1 (str): First sentence\n",
    "      s2 (str): Second sentence\n",
    "    \n",
    "    Return:\n",
    "      int: The number of common words that the two sentences have in common\n",
    "    '''\n",
    "    # Tokenize\n",
    "    tokens1 = set(word_tokenize(s1))\n",
    "    tokens2 = set(word_tokenize(s2))\n",
    "    \n",
    "    common = tokens1 & tokens2 # list of common tokens\n",
    "    return len(common)\n",
    "\n",
    "\n",
    "# Number of common words in the same position\n",
    "def number_common_words_2(s1, s2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s1 (str): The first input sentence.\n",
    "      s2 (str): The second input sentence.\n",
    "\n",
    "    Returns:\n",
    "      int: The number of common words at the same position in both sentences.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(s1)\n",
    "    tokens2 = word_tokenize(s2)\n",
    "\n",
    "    min_length = min(len(tokens1), len(tokens2))\n",
    "\n",
    "    # Common words at the same position\n",
    "    common_count = 0\n",
    "    for i in range(min_length):\n",
    "        if tokens1[i].lower() == tokens2[i].lower():\n",
    "            common_count += 1\n",
    "\n",
    "    return common_count\n",
    "\n",
    "\n",
    "# If the first word of two sentences is equal\n",
    "def first_word_equal(s1, s2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s1 (str): First sentence\n",
    "      s2 (str): Second sentence\n",
    "    Returns:\n",
    "      A binary value indicating whether the firsts words of the two questions are equal.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(s1)\n",
    "    tokens2 = word_tokenize(s2)\n",
    "    \n",
    "    if tokens1[0].lower() == tokens2[0].lower():\n",
    "            return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# If the last word of two sentences is equal\n",
    "def last_word_equal(s1, s2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s1 (str): First sentence\n",
    "      s2 (str): Second sentence\n",
    "    Returns:\n",
    "      A binary value indicating whether the lasts words of the two questions are equal.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens1 = word_tokenize(s1)\n",
    "    tokens2 = word_tokenize(s2) # with word_tokenize, counts '.' as different token\n",
    "    \n",
    "    if tokens1[-1].lower() == tokens2[-1].lower():\n",
    "            return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b585e",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b01f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence 1:\n",
      "\t This is an example sentence to test the count of words\n",
      "Original sentence 2:\n",
      "\t This is a second example to test the count of common words\n",
      "Original sentence 3:\n",
      "\t Another different sentence\n",
      "\n",
      "Number of words of sentence 1: 11\n",
      "\n",
      "Number of words of sentence 2: 12\n",
      "\n",
      "Number of common words between sentence 1 and sentence 2: 9\n",
      "\n",
      "Number of common words between sentence 1 and sentence 3: 1\n",
      "\n",
      "Number of common words in the same position between sentence 1 and sentence 2: 7\n",
      "\n",
      "Number of common words in the same position between sentence 1 and sentence 3: 0\n",
      "\n",
      "Comparing the first words of sentence1 and sentence2: 1\n",
      "\n",
      "Comparing the first words of sentence1 and sentence3: 0\n",
      "\n",
      "Comparing the last words of sentence1 and sentence2: 1\n",
      "\n",
      "Comparing the last words of sentence1 and sentence3: 0\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'This is an example sentence to test the count of words'\n",
    "sentence2 = 'This is a second example to test the count of common words'\n",
    "sentence3 = 'Another different sentence'\n",
    "print(\"Original sentence 1:\\n\\t\", sentence1)\n",
    "print(\"Original sentence 2:\\n\\t\", sentence2)\n",
    "print(\"Original sentence 3:\\n\\t\", sentence3)\n",
    "\n",
    "# number of words\n",
    "k1 = number_words(sentence1)\n",
    "k2 = number_words(sentence2)\n",
    "print(\"\\nNumber of words of sentence 1:\", k1)\n",
    "print(\"\\nNumber of words of sentence 2:\", k2)\n",
    "\n",
    "# number of common words\n",
    "k3 = number_common_words(sentence1, sentence2)\n",
    "print(\"\\nNumber of common words between sentence 1 and sentence 2:\", k3)\n",
    "\n",
    "k4 = number_common_words(sentence1, sentence3)\n",
    "print(\"\\nNumber of common words between sentence 1 and sentence 3:\", k4)\n",
    "\n",
    "# number of common words in the same postion\n",
    "k3 = number_common_words_2(sentence1, sentence2)\n",
    "print(\"\\nNumber of common words in the same position between sentence 1 and sentence 2:\", k3)\n",
    "\n",
    "k4 = number_common_words_2(sentence1, sentence3)\n",
    "print(\"\\nNumber of common words in the same position between sentence 1 and sentence 3:\", k4)\n",
    "\n",
    "# first and last words equal\n",
    "print(\"\\nComparing the first words of sentence1 and sentence2:\", first_word_equal(sentence1,sentence2))\n",
    "print(\"\\nComparing the first words of sentence1 and sentence3:\", first_word_equal(sentence1,sentence3))\n",
    "\n",
    "print(\"\\nComparing the last words of sentence1 and sentence2:\", last_word_equal(sentence1,sentence2))\n",
    "print(\"\\nComparing the last words of sentence1 and sentence3:\", last_word_equal(sentence1,sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eccc9c",
   "metadata": {},
   "source": [
    "## Models with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceaa24a",
   "metadata": {},
   "source": [
    "We will apply our preprocessing functions to the data and construct the same model as in the simple solution to see if the performance is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f130081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "854d1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_quora = '../nlp_deliv1_materials/'\n",
    "\n",
    "# Train and Validation data\n",
    "train_df = pd.read_csv(os.path.join(path_folder_quora, \"quora_train_data.csv\"))\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(os.path.join(path_folder_quora,\"quora_test_data.csv\"))\n",
    "\n",
    "A_df, te_df = sklearn.model_selection.train_test_split(train_df, test_size=0.05, random_state=123)\n",
    "tr_df, va_df = sklearn.model_selection.train_test_split(A_df, test_size=0.05, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "341d5e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      " X train (291897, 5)\n",
      " y train (291897,)\n",
      " --------------------\n",
      "Validation:\n",
      " X val (15363, 5)\n",
      " y val (15363,)\n",
      " --------------------\n",
      "Test:\n",
      " X test (16172, 5)\n",
      " y test (16172,)\n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "# dividng X and y for each dataset\n",
    "y_tr = tr_df['is_duplicate'].values\n",
    "X_tr_df = tr_df.drop(['is_duplicate'], axis =1)\n",
    "\n",
    "y_va = va_df['is_duplicate'].values\n",
    "X_va_df = va_df.drop(['is_duplicate'], axis =1)\n",
    "\n",
    "y_te = te_df['is_duplicate'].values\n",
    "X_te_df = te_df.drop(['is_duplicate'], axis =1)\n",
    "\n",
    "print(f'Training:\\n X train {X_tr_df.shape}\\n y train {y_tr.shape}\\n {\"-\"*20}')\n",
    "print(f'Validation:\\n X val {X_va_df.shape}\\n y val {y_va.shape}\\n {\"-\"*20}')\n",
    "print(f'Test:\\n X test {X_te_df.shape}\\n y test {y_te.shape}\\n {\"-\"*20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "af5fa577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input data into list of strings\n",
    "\n",
    "q1_train =  cast_list_as_strings(list(X_tr_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(X_tr_df[\"question2\"]))\n",
    "\n",
    "q1_val =  cast_list_as_strings(list(X_va_df[\"question1\"]))\n",
    "q2_val =  cast_list_as_strings(list(X_va_df[\"question2\"]))\n",
    "\n",
    "q1_test =  cast_list_as_strings(list(X_te_df[\"question1\"]))\n",
    "q2_test =  cast_list_as_strings(list(X_te_df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71bb2d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Java or C++ or C the most popular language amongst startups for backend development?\n",
      "How do I develop a software which will have a Java GUI and a C++ or C backend?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "print(q1_train[0])\n",
    "print(q2_train[0])\n",
    "print(y_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72b523",
   "metadata": {},
   "source": [
    "### Apply preprocessing functions to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac33b0",
   "metadata": {},
   "source": [
    "We apply the more agressive function 'remove_special_characters' as well as 'remove_stopwords' and 'normalize_spaces' to test the model with the simplified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0702e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data with the function remove_special_characters that is the more agressive function\n",
    "# also remove stopwords and normalize spaces\n",
    "\n",
    "def preprocess_data(question_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      question_list (str list): list of string questions\n",
    "    Returns:\n",
    "      The list after preprocessing (we apply the preprocessing functions).\n",
    "    \"\"\"\n",
    "    q_lower = [lowercase_sentence(question) for question in question_list] # lovercase\n",
    "    q_sc = [remove_special_characters(question) for question in q_lower] # remove special characters (all)\n",
    "    q_sw = [remove_stopwords(question) for question in q_sc] # remove stop words\n",
    "    q_preprocessed = [normalize_spaces(question) for question in q_sw] # normalize spaces\n",
    "    return q_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dad73923",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_preprocessed = preprocess_data(q1_train)\n",
    "q2_train_preprocessed = preprocess_data(q2_train)\n",
    "q1_val_preprocessed = preprocess_data(q1_val)\n",
    "q2_val_preprocessed = preprocess_data(q2_val)\n",
    "q1_test_preprocessed = preprocess_data(q1_test)\n",
    "q2_test_preprocessed = preprocess_data(q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "abeac9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java c c most popular language amongst startups backend development\n",
      "how do i develop a software which will a java gui a c c backend\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "print(q1_train_preprocessed[0])\n",
    "print(q2_train_preprocessed[0])\n",
    "print(y_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d73cff",
   "metadata": {},
   "source": [
    "### Use a countvectorizer like in the simple solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fd1a2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the countvectorizer\n",
    "all_q_train_preprocessed = q1_train_preprocessed+q2_train_preprocessed\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(all_q_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8e101eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with preprocessed columns\n",
    "X_tr_pre = pd.DataFrame({'question1': q1_train_preprocessed, 'question2': q2_train_preprocessed}, columns=['question1', 'question2'])\n",
    "#print(X_tr_pre)\n",
    "X_va_pre = pd.DataFrame({'question1': q1_val_preprocessed, 'question2': q2_val_preprocessed}, columns=['question1', 'question2'])\n",
    "X_te_pre = pd.DataFrame({'question1': q1_test_preprocessed, 'question2': q2_test_preprocessed}, columns=['question1', 'question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2998eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features (concatenating q1+q2)\n",
    "X_tr_q1q2_pre = get_features_from_df(X_tr_pre, count_vectorizer) # it converts list as strings and performs count_vectorizer\n",
    "X_va_q1q2_pre = get_features_from_df(X_va_pre, count_vectorizer)\n",
    "X_te_q1q2_pre = get_features_from_df(X_te_pre, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68ffcd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<291897x148172 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4253758 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4f9c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      " X train (291897, 148172)\n",
      " --------------------\n",
      "Validation:\n",
      " X val (15363, 148172)\n",
      "--------------------\n",
      "Test:\n",
      " X test (16172, 148172)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(f'Training:\\n X train {X_tr_q1q2_pre.shape}\\n {\"-\"*20}')\n",
    "print(f'Validation:\\n X val {X_va_q1q2_pre.shape}\\n{\"-\"*20}')\n",
    "print(f'Test:\\n X test {X_te_q1q2_pre.shape}\\n{\"-\"*20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2201bb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training a simple model\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic.fit(X_tr_q1q2_pre, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "154f0331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8121803238813691,\n",
       " 'roc_auc': 0.7844546064505284,\n",
       " 'precision': 0.7827302825427779,\n",
       " 'recall': 0.6789375365942063,\n",
       " 'f1': 0.7271487582740258}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train metrics\n",
    "train_metrics = evaluate_model(X_tr_q1q2_pre, y_tr, model=logistic, display=False)\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "964a6510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7461433313805897,\n",
       " 'roc_auc': 0.7163589696383614,\n",
       " 'precision': 0.6744323790720632,\n",
       " 'recall': 0.6027880712899242,\n",
       " 'f1': 0.636600819977637}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation results\n",
    "validation_metrics = evaluate_model(X_va_q1q2_pre, y_va, model=logistic, display=False)\n",
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0f7d1d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7556270096463023,\n",
       " 'roc_auc': 0.7260376912293912,\n",
       " 'precision': 0.6944181646168401,\n",
       " 'recall': 0.6109538871316798,\n",
       " 'f1': 0.6500177116542686}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test results\n",
    "test_metrics  = evaluate_model(X_te_q1q2_pre, y_te, model=logistic, display=False)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99657e7",
   "metadata": {},
   "source": [
    "**Conclusion**: after preprocessing the data, we obtain a very similar result to the simple solution. We will to use it to obtain a more homogenic data and because other models need it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f2c98",
   "metadata": {},
   "source": [
    "### Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3355a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset with correct features:\n",
    "\n",
    "# Save as model_name+(X/y)+(tr/va/te) (depending if its dataset or labels and what type they are)\n",
    "\n",
    "# save model\n",
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "if not os.path.isdir(\"model/simple_solution_pre\"):\n",
    "        os.mkdir(\"model/simple_solution_pre\")\n",
    "        \n",
    "with open('model/simple_solution_pre/simple_model_pre.pkl','wb') as f:\n",
    "    pickle.dump(logistic,f)\n",
    "\n",
    "with open('model/simple_solution_pre/simple_model_pre_X_tr.pkl','wb') as f:\n",
    "    pickle.dump(X_tr_q1q2_pre,f)  \n",
    "with open('model/simple_solution_pre/simple_model_pre_X_va.pkl','wb') as f:\n",
    "    pickle.dump(X_va_q1q2_pre,f)   \n",
    "with open('model/simple_solution_pre/simple_model_pre_X_te.pkl','wb') as f:\n",
    "    pickle.dump(X_te_q1q2_pre,f)\n",
    "with open('model/simple_solution_pre/simple_model_pre_y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr,f)\n",
    "with open('model/simple_solution_pre/simple_model_pre_y_va.pkl','wb') as f:\n",
    "    pickle.dump(y_va,f)\n",
    "with open('model/simple_solution_pre/simple_model_pre_y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db123fc2",
   "metadata": {},
   "source": [
    "## Models with text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa7efe",
   "metadata": {},
   "source": [
    "We will construct a more extensive dataframe with the basic text features defined above to see if the performance from the simple solution improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5dd1fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_numeric_features(q1_list, q2_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      q1_list (str list): list of string questions\n",
    "      q2_list (str list): list of string questions\n",
    "    Returns:\n",
    "      A data frame containing the text features applied to both lists.\n",
    "    \"\"\"\n",
    "    # number of words\n",
    "    q1_f1 = [number_words(question) for question in q1_list]\n",
    "    q2_f1 = [number_words(question) for question in q2_list]\n",
    "    \n",
    "    # number of common words\n",
    "    q1q2_f2 = [number_common_words(question1, question2) for question1, question2 in zip(q1_list, q2_list)]\n",
    "    \n",
    "    # number of common words in the same position\n",
    "    q1q2_f3 = [number_common_words_2(question1, question2) for question1, question2 in zip(q1_list, q2_list)]\n",
    "    \n",
    "    # first word equal\n",
    "    q1q2_f4 = [first_word_equal(question1, question2) for question1, question2 in zip(q1_list, q2_list)]\n",
    "    \n",
    "    # last word equal\n",
    "    q1q2_f5 = [last_word_equal(question1, question2) for question1, question2 in zip(q1_list, q2_list)]\n",
    "    \n",
    "    # build dataframe with features\n",
    "    df_features = pd.DataFrame({'num_words_1': q1_f1, 'num_words_2': q2_f1, 'num_common_words': q1q2_f2,\n",
    "                               'num_common_words_2': q1q2_f3, 'first_word': q1q2_f4, 'last_word': q1q2_f5}, \n",
    "                               columns=['num_words_1', 'num_words_2', 'num_common_words', 'num_common_words_2', \n",
    "                                        'first_word', 'last_word'])\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "85fedea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words_1</th>\n",
       "      <th>num_words_2</th>\n",
       "      <th>num_common_words</th>\n",
       "      <th>num_common_words_2</th>\n",
       "      <th>first_word</th>\n",
       "      <th>last_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291892</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291893</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291894</th>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291895</th>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291896</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291897 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_words_1  num_words_2  num_common_words  num_common_words_2  \\\n",
       "0                16           19                 6                   0   \n",
       "1                16           27                 2                   0   \n",
       "2                 8            8                 7                   7   \n",
       "3                16           11                 6                   7   \n",
       "4                 5            8                 2                   1   \n",
       "...             ...          ...               ...                 ...   \n",
       "291892           13           21                 9                   3   \n",
       "291893            4            5                 4                   1   \n",
       "291894           16           23                 5                   1   \n",
       "291895           17           16                 2                   0   \n",
       "291896            8           13                 5                   7   \n",
       "\n",
       "        first_word  last_word  \n",
       "0                0          1  \n",
       "1                0          1  \n",
       "2                1          1  \n",
       "3                1          0  \n",
       "4                1          1  \n",
       "...            ...        ...  \n",
       "291892           1          1  \n",
       "291893           1          1  \n",
       "291894           0          1  \n",
       "291895           0          1  \n",
       "291896           1          1  \n",
       "\n",
       "[291897 rows x 6 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_features = build_numeric_features(q1_train, q2_train)\n",
    "X_tr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "61207194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv \n",
    "X_tr_features.to_csv('X_tr_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bb45a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine countvectorizer data with features\n",
    "X_tr_features_sparse = scipy.sparse.hstack([X_tr_q1q2_pre, scipy.sparse.csr_matrix(X_tr_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a628abdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic2 = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic2.fit(X_tr_features_sparse, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14a51f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8380627413094345,\n",
       " 'roc_auc': 0.8221126055607623,\n",
       " 'precision': 0.7913817085893127,\n",
       " 'recall': 0.7614104220299445,\n",
       " 'f1': 0.7761068192475475}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train metrics\n",
    "train_metrics2 = evaluate_model(X_tr_features_sparse, y_tr, model=logistic2, display=False)\n",
    "train_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e6a113ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words_1</th>\n",
       "      <th>num_words_2</th>\n",
       "      <th>num_common_words</th>\n",
       "      <th>num_common_words_2</th>\n",
       "      <th>first_word</th>\n",
       "      <th>last_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15358</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15359</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15360</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15361</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15362</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15363 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_words_1  num_words_2  num_common_words  num_common_words_2  \\\n",
       "0               11           13                 6                   6   \n",
       "1               10            6                 3                   2   \n",
       "2                7           20                 5                   0   \n",
       "3                4            4                 3                   3   \n",
       "4               12           11                 5                   1   \n",
       "...            ...          ...               ...                 ...   \n",
       "15358           11           11                10                  10   \n",
       "15359           16            8                 5                   0   \n",
       "15360            6            5                 4                   3   \n",
       "15361            8           18                 2                   0   \n",
       "15362            7            7                 6                   6   \n",
       "\n",
       "       first_word  last_word  \n",
       "0               0          1  \n",
       "1               0          1  \n",
       "2               0          1  \n",
       "3               1          1  \n",
       "4               1          1  \n",
       "...           ...        ...  \n",
       "15358           1          1  \n",
       "15359           0          1  \n",
       "15360           1          1  \n",
       "15361           0          1  \n",
       "15362           1          1  \n",
       "\n",
       "[15363 rows x 6 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_va_features = build_numeric_features(q1_val, q2_val)\n",
    "X_va_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e086bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv \n",
    "X_va_features.to_csv('X_va_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6ae238b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7801861615569876,\n",
       " 'roc_auc': 0.7579938330651488,\n",
       " 'precision': 0.7143391988019469,\n",
       " 'recall': 0.6733721545791425,\n",
       " 'f1': 0.6932509764737943}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine countvectorizer data with features\n",
    "X_va_features_sparse = scipy.sparse.hstack([X_va_q1q2_pre, scipy.sparse.csr_matrix(X_va_features)])\n",
    "\n",
    "# evaluate validation\n",
    "train_metrics2 = evaluate_model(X_va_features_sparse, y_va, model=logistic2, display=False)\n",
    "train_metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f5c11130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words_1</th>\n",
       "      <th>num_words_2</th>\n",
       "      <th>num_common_words</th>\n",
       "      <th>num_common_words_2</th>\n",
       "      <th>first_word</th>\n",
       "      <th>last_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16167</th>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16168</th>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16169</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16170</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16171</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16172 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_words_1  num_words_2  num_common_words  num_common_words_2  \\\n",
       "0                9           12                 9                   4   \n",
       "1               15           10                 4                   0   \n",
       "2               20           17                 6                   0   \n",
       "3               10            9                 6                   1   \n",
       "4               12           11                 3                   0   \n",
       "...            ...          ...               ...                 ...   \n",
       "16167           14           28                 3                   0   \n",
       "16168           14           32                 2                   0   \n",
       "16169            9           10                 3                   2   \n",
       "16170            7           11                 6                   1   \n",
       "16171            8            7                 5                   0   \n",
       "\n",
       "       first_word  last_word  \n",
       "0               1          1  \n",
       "1               0          1  \n",
       "2               0          1  \n",
       "3               1          1  \n",
       "4               0          1  \n",
       "...           ...        ...  \n",
       "16167           0          1  \n",
       "16168           0          1  \n",
       "16169           1          1  \n",
       "16170           1          1  \n",
       "16171           0          1  \n",
       "\n",
       "[16172 rows x 6 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te_features = build_numeric_features(q1_test, q2_test)\n",
    "X_te_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "93c7a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv \n",
    "X_te_features.to_csv('X_te_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "abac8ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.785864457086322,\n",
       " 'roc_auc': 0.7652421003172968,\n",
       " 'precision': 0.7237073513893775,\n",
       " 'recall': 0.685034126852006,\n",
       " 'f1': 0.7038399042161978}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine countvectorizer data with features\n",
    "X_te_features_sparse = scipy.sparse.hstack([X_te_q1q2_pre, scipy.sparse.csr_matrix(X_te_features)])\n",
    "\n",
    "# evaluate validation\n",
    "train_metrics2 = evaluate_model(X_te_features_sparse, y_te, model=logistic2, display=False)\n",
    "train_metrics2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ad681",
   "metadata": {},
   "source": [
    "**Conclusion:** with the features the performance is improved **about 3%-5%** (not much), but something. We decide to keep the dataset with features and test with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc32d2e",
   "metadata": {},
   "source": [
    "### Save features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53ac9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset with correct features:\n",
    "\n",
    "# Save as model_name+(X/y)+(tr/va/te) (depending if its dataset or labels and what type they are)\n",
    "\n",
    "# save model\n",
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "if not os.path.isdir(\"model/features_solution\"):\n",
    "        os.mkdir(\"model/features_solution\")\n",
    "        \n",
    "with open('model/features_solution/features_model.pkl','wb') as f:\n",
    "    pickle.dump(logistic2,f)\n",
    "\n",
    "with open('model/features_solution/features_model_X_tr.pkl','wb') as f:\n",
    "    pickle.dump(X_tr_features_sparse,f)  \n",
    "with open('model/features_solution/features_model_X_va.pkl','wb') as f:\n",
    "    pickle.dump(X_va_features_sparse,f)   \n",
    "with open('model/features_solution/features_model_X_te.pkl','wb') as f:\n",
    "    pickle.dump(X_te_features_sparse,f)\n",
    "with open('model/features_solution/features_model_y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr,f)\n",
    "with open('model/features_solution/features_model_y_va.pkl','wb') as f:\n",
    "    pickle.dump(y_va,f)\n",
    "with open('model/features_solution/features_model_y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f5546",
   "metadata": {},
   "source": [
    "## Test different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276f1b5",
   "metadata": {},
   "source": [
    "Now that we know that the performance of the basic model is improved when adding features, we want to test more complex classifiers to see if they produce better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "366cb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49db534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "def train_models(X_train, y_train):\n",
    "    rf_model = RandomForestClassifier(max_depth = 5, random_state=123)\n",
    "    xgb_model = XGBClassifier(random_state=123)\n",
    "    svc_model = SVC(random_state=123)\n",
    "\n",
    "    classifiers = [rf_model, xgb_model, svc_model]\n",
    "    trained_clf = []\n",
    "\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_tr_features_sparse, y_tr)\n",
    "        clf.append(trained_clf)\n",
    "        \n",
    "    return trained_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2920a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained classifiers\n",
    "for clf in trained_clf:\n",
    "    metrics = evaluate_model(X_test, y_test, clf, display=False)\n",
    "    print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
