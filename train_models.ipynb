{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVwnS93T30n-"
   },
   "source": [
    "# Train Models\n",
    "<div style=\"background-color: lightblue; padding: 10px; border-radius: 10px;\">\n",
    "\n",
    "**IMPORTANT INFO:**\n",
    "\n",
    "The `train_models.ipynb` notebook:\n",
    "- Is a responsability of all members of a group. All of you should execute this and ensure it works as expected.\n",
    "- Has to use the code done by each member in the group to generate features for the challenge.\n",
    "\n",
    "\n",
    "`models`: A folder containing the trained models. This folder should be cre- ated by `train_models.ipynb` and models should be stored there after running `train_models.ipynb` notebook. The code should check if the folder is there and in such a case do not overwrite/store the models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_fFMZVYSjsx"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JoW1tYx-R943"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9cUIGQH4Gku"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkWWR7NwR-2W"
   },
   "source": [
    "From the problem guide the teacher says:\n",
    "\n",
    "This is a Kaggle challenge: There is no validation/test data with labels.\n",
    "Therefore you have to create the following split in order to share the same train validation and test splits across teams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HBPAHqjdR2il"
   },
   "outputs": [],
   "source": [
    "path_folder_quora = '../nlp_deliv1_materials/'\n",
    "\n",
    "# Train and Validation data\n",
    "train_df = pd.read_csv(os.path.join(path_folder_quora, \"quora_train_data.csv\"))\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(os.path.join(path_folder_quora,\"quora_test_data.csv\"))\n",
    "\n",
    "A_df, te_df = sklearn.model_selection.train_test_split(train_df, test_size=0.05, random_state=123)\n",
    "tr_df, va_df = sklearn.model_selection.train_test_split(A_df, test_size=0.05, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1713111794742,
     "user": {
      "displayName": "Clàudia Valverde Sanchez",
      "userId": "17089465752638224867"
     },
     "user_tz": -120
    },
    "id": "ae2vT4FH-4Gw",
    "outputId": "e9c60cc8-5d67-43b1-d1c9-77dd6c3f6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      " X train (291897, 5)\n",
      " y train (291897,)\n",
      " --------------------\n",
      "Validation:\n",
      " X val (15363, 5)\n",
      " y val (15363,)\n",
      " --------------------\n",
      "Test:\n",
      " X test (16172, 5)\n",
      " y test (16172,)\n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "# dividng X and y for each dataset\n",
    "y_tr = tr_df['is_duplicate'].values\n",
    "X_tr_df = tr_df.drop(['is_duplicate'], axis =1)\n",
    "\n",
    "y_va = va_df['is_duplicate'].values\n",
    "X_va_df = va_df.drop(['is_duplicate'], axis =1)\n",
    "\n",
    "y_te = te_df['is_duplicate'].values\n",
    "X_te_df = te_df.drop(['is_duplicate'], axis =1)\n",
    "\n",
    "print(f'Training:\\n X train {X_tr_df.shape}\\n y train {y_tr.shape}\\n {\"-\"*20}')\n",
    "print(f'Validation:\\n X val {X_va_df.shape}\\n y val {y_va.shape}\\n {\"-\"*20}')\n",
    "print(f'Test:\\n X test {X_te_df.shape}\\n y test {y_te.shape}\\n {\"-\"*20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za89utw14LAP"
   },
   "source": [
    "# Simple Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z1CLM54X4Vwp"
   },
   "outputs": [],
   "source": [
    "# convert input data into list of strings\n",
    "\n",
    "q1_train =  cast_list_as_strings(list(X_tr_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(X_tr_df[\"question2\"]))\n",
    "\n",
    "q1_val =  cast_list_as_strings(list(X_va_df[\"question1\"]))\n",
    "q2_val =  cast_list_as_strings(list(X_va_df[\"question2\"]))\n",
    "\n",
    "q1_test =  cast_list_as_strings(list(X_te_df[\"question1\"]))\n",
    "q2_test =  cast_list_as_strings(list(X_te_df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFsPvvQbNwqW"
   },
   "source": [
    "Use all the questions in train and test partitions to build a single list all_questions to fit the count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 8830,
     "status": "ok",
     "timestamp": 1713111804161,
     "user": {
      "displayName": "Clàudia Valverde Sanchez",
      "userId": "17089465752638224867"
     },
     "user_tz": -120
    },
    "id": "Ws2nWF8-MdwI",
    "outputId": "94761172-bfc1-479c-d674-7d0e5b409ec1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_q_train = q1_train+q2_train\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(all_q_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "57sFuKVxNVnM"
   },
   "outputs": [],
   "source": [
    "# get features (concatenating q1+q2)\n",
    "X_tr_q1q2 = get_features_from_df(X_tr_df, count_vectorizer) # it converts list as strings and performs count_vectorizer\n",
    "X_va_q1q2 = get_features_from_df(X_va_df, count_vectorizer)\n",
    "X_te_q1q2 = get_features_from_df(X_te_df, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1713111814432,
     "user": {
      "displayName": "Clàudia Valverde Sanchez",
      "userId": "17089465752638224867"
     },
     "user_tz": -120
    },
    "id": "x9raLtLDV4fU",
    "outputId": "a0b0755e-1558-4f08-a557-08b18ae1885d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      " X train (291897, 149650)\n",
      " --------------------\n",
      "Validation:\n",
      " X val (15363, 149650)\n",
      "--------------------\n",
      "Test:\n",
      " X test (16172, 149650)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(f'Training:\\n X train {X_tr_q1q2.shape}\\n {\"-\"*20}')\n",
    "print(f'Validation:\\n X val {X_va_q1q2.shape}\\n{\"-\"*20}')\n",
    "print(f'Test:\\n X test {X_te_q1q2.shape}\\n{\"-\"*20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 119476,
     "status": "ok",
     "timestamp": 1713111933902,
     "user": {
      "displayName": "Clàudia Valverde Sanchez",
      "userId": "17089465752638224867"
     },
     "user_tz": -120
    },
    "id": "O1-7XogxV7WV",
    "outputId": "509cb760-583c-4fe5-b9c6-311216ca1432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training a simple model\n",
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",\n",
    "                                                   random_state=123)\n",
    "logistic.fit(X_tr_q1q2, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTbjV4FR9iSc"
   },
   "source": [
    "### Saving simple model\n",
    "Creating model folder + saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-Xd8-5YVVxrR"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "if not os.path.isdir(\"model/simple_solution\"):\n",
    "        os.mkdir(\"model/simple_solution\")\n",
    "        \n",
    "with open('model/simple_solution/simple_model.pkl','wb') as f:\n",
    "    pickle.dump(logistic,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset with correct features:\n",
    "\n",
    "# Save as model_name+(X/y)+(tr/va/te) (depending if its dataset or lavels and what type they are)\n",
    "\n",
    "with open('model/simple_solution/simple_model_X_tr.pkl','wb') as f:\n",
    "    pickle.dump(X_tr_q1q2,f)  \n",
    "with open('model/simple_solution/simple_model_X_va.pkl','wb') as f:\n",
    "    pickle.dump(X_va_q1q2,f)   \n",
    "with open('model/simple_solution/simple_model_X_te.pkl','wb') as f:\n",
    "    pickle.dump(X_te_q1q2,f)\n",
    "with open('model/simple_solution/simple_model_y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr,f)\n",
    "with open('model/simple_solution/simple_model_y_va.pkl','wb') as f:\n",
    "    pickle.dump(y_va,f)\n",
    "with open('model/simple_solution/simple_model_y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HOWSSEk4R1P"
   },
   "source": [
    "# Improved Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Basic Text Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fisrt approach is to perform some text cleaning and add some basic text features (both are defined in `utils_Alba.ipynb`). We have also tested different classifiers but the logistic model obtains the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY PREPROCESSING TO THE DATA\n",
    "\n",
    "q1_train_preprocessed = preprocess_data(q1_train)\n",
    "q2_train_preprocessed = preprocess_data(q2_train)\n",
    "q1_val_preprocessed = preprocess_data(q1_val)\n",
    "q2_val_preprocessed = preprocess_data(q2_val)\n",
    "q1_test_preprocessed = preprocess_data(q1_test)\n",
    "q2_test_preprocessed = preprocess_data(q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the countvectorizer like in the simple solution\n",
    "all_q_train_preprocessed = q1_train_preprocessed+q2_train_preprocessed\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(all_q_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with preprocessed columns\n",
    "X_tr_pre = pd.DataFrame({'question1': q1_train_preprocessed, 'question2': q2_train_preprocessed}, columns=['question1', 'question2'])\n",
    "#print(X_tr_pre)\n",
    "X_va_pre = pd.DataFrame({'question1': q1_val_preprocessed, 'question2': q2_val_preprocessed}, columns=['question1', 'question2'])\n",
    "X_te_pre = pd.DataFrame({'question1': q1_test_preprocessed, 'question2': q2_test_preprocessed}, columns=['question1', 'question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features (concatenating q1+q2)\n",
    "X_tr_q1q2_pre = get_features_from_df(X_tr_pre, count_vectorizer) # it converts list as strings and performs count_vectorizer\n",
    "X_va_q1q2_pre = get_features_from_df(X_va_pre, count_vectorizer)\n",
    "X_te_q1q2_pre = get_features_from_df(X_te_pre, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT DataFrame WITH FEATURES\n",
    "\n",
    "X_tr_features = build_numeric_features(q1_train, q2_train)\n",
    "X_va_features = build_numeric_features(q1_val, q2_val)\n",
    "X_te_features = build_numeric_features(q1_test, q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine count vectorizer data with features in sparse form\n",
    "\n",
    "X_tr_features_sparse = scipy.sparse.hstack([X_tr_q1q2_pre, scipy.sparse.csr_matrix(X_tr_features)])\n",
    "X_va_features_sparse = scipy.sparse.hstack([X_va_q1q2_pre, scipy.sparse.csr_matrix(X_va_features)])\n",
    "X_te_features_sparse = scipy.sparse.hstack([X_te_q1q2_pre, scipy.sparse.csr_matrix(X_te_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN LOGISTIC MODEL\n",
    "\n",
    "logistic2 = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic2.fit(X_tr_features_sparse, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND DATASET\n",
    "\n",
    "# save model\n",
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "if not os.path.isdir(\"model/features_solution\"):\n",
    "        os.mkdir(\"model/features_solution\")\n",
    "        \n",
    "with open('model/features_solution/features_model.pkl','wb') as f:\n",
    "    pickle.dump(logistic2,f)\n",
    "    \n",
    "# save data\n",
    "with open('model/features_solution/features_model_X_tr.pkl','wb') as f:\n",
    "    pickle.dump(X_tr_features_sparse,f)  \n",
    "with open('model/features_solution/features_model_X_va.pkl','wb') as f:\n",
    "    pickle.dump(X_va_features_sparse,f)   \n",
    "with open('model/features_solution/features_model_X_te.pkl','wb') as f:\n",
    "    pickle.dump(X_te_features_sparse,f)\n",
    "with open('model/features_solution/features_model_y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr,f)\n",
    "with open('model/features_solution/features_model_y_va.pkl','wb') as f:\n",
    "    pickle.dump(y_va,f)\n",
    "with open('model/features_solution/features_model_y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate embeddings (word2vec, TF-IDF, ...) + train more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "vec = get_Word2Vect_from_clean(pd.DataFrame({'q1': q1_train_preprocessed, 'q2':q2_train_preprocessed}))\n",
    "dfw2v = pd.DataFrame({'q1':vec[0],'q2':vec[1],'dup':y_tr})\n",
    "dfw2v.dropna(inplace=True)\n",
    "y_tr_w2v = dfw2v['dup'].values\n",
    "X_tr_w2v = np.hstack((np.array([x for x in dfw2v['q1'].values]),np.array([x for x in dfw2v['q2'].values])))\n",
    "#Validation\n",
    "vec = get_Word2Vect_from_clean(pd.DataFrame({'q1': q1_val_preprocessed, 'q2':q2_val_preprocessed}))\n",
    "dfw2v = pd.DataFrame({'q1':vec[0],'q2':vec[1],'dup':y_va})\n",
    "dfw2v.dropna(inplace=True)\n",
    "y_va_w2v = dfw2v['dup'].values\n",
    "X_va_w2v = np.hstack((np.array([x for x in dfw2v['q1'].values]),np.array([x for x in dfw2v['q2'].values])))\n",
    "#Test\n",
    "vec = get_Word2Vect_from_clean(pd.DataFrame({'q1': q1_test_preprocessed, 'q2':q2_test_preprocessed}))\n",
    "dfw2v = pd.DataFrame({'q1':vec[0],'q2':vec[1],'dup':y_te})\n",
    "dfw2v.dropna(inplace=True)\n",
    "y_te_w2v = dfw2v['dup'].values\n",
    "X_te_w2v = np.hstack((np.array([x for x in dfw2v['q1'].values]),np.array([x for x in dfw2v['q2'].values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic3 = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic3.fit(X_tr_w2v, y_tr_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND DATASET\n",
    "folder = 'word2vect'\n",
    "# save model\n",
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "if not os.path.isdir(f\"model/{folder}\"):\n",
    "        os.mkdir(f\"model/{folder}\")\n",
    "        \n",
    "with open(f\"model/{folder}_solution/{folder}_model.pkl\",'wb') as f:\n",
    "    pickle.dump(logistic3,f)\n",
    "    \n",
    "# save data\n",
    "with open(f'model/{folder}_solution/{folder}_model_X_tr.pkl','wb') as f:\n",
    "    pickle.dump(X_tr_w2v,f)  \n",
    "with open(f'model/{folder}_solution/{folder}_model_X_va.pkl','wb') as f:\n",
    "    pickle.dump(X_va_w2v,f)   \n",
    "with open(f'model/{folder}_solution/{folder}_model_X_te.pkl','wb') as f:\n",
    "    pickle.dump(X_te_w2v,f)\n",
    "with open(f'model/{folder}_solution/{folder}_model_y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr_w2v,f)\n",
    "with open(f'model/{folder}_solution/{folder}_model_y_va.pkl','wb') as f:\n",
    "    pickle.dump(y_va_w2v,f)\n",
    "with open(f'model/{folder}_solution/{folder}_model_y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te_w2v,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vect pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pretrained word2vec model\n",
    "pre = KeyedVectors.load(\"model/word2vect/pretrained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\anaconda3\\envs\\quora_challenge_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Alejandro\\anaconda3\\envs\\quora_challenge_env\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "vec = get_Word2Vect_from_clean(pd.DataFrame({'q1': q1_train_preprocessed, 'q2':q2_train_preprocessed}), pre)\n",
    "dfw2v = pd.DataFrame({'q1':vec[0],'q2':vec[1],'dup':y_tr})\n",
    "dfw2v.dropna(inplace=True)\n",
    "y_tr_w2v = dfw2v['dup'].values\n",
    "X_tr_w2v = np.hstack((np.array([x for x in dfw2v['q1'].values]),np.array([x for x in dfw2v['q2'].values])))\n",
    "#Validation\n",
    "vec = get_Word2Vect_from_clean(pd.DataFrame({'q1': q1_val_preprocessed, 'q2':q2_val_preprocessed}), pre)\n",
    "dfw2v = pd.DataFrame({'q1':vec[0],'q2':vec[1],'dup':y_va})\n",
    "dfw2v.dropna(inplace=True)\n",
    "y_va_w2v = dfw2v['dup'].values\n",
    "X_va_w2v = np.hstack((np.array([x for x in dfw2v['q1'].values]),np.array([x for x in dfw2v['q2'].values])))\n",
    "#Test\n",
    "vec = get_Word2Vect_from_clean(pd.DataFrame({'q1': q1_test_preprocessed, 'q2':q2_test_preprocessed}), pre)\n",
    "dfw2v = pd.DataFrame({'q1':vec[0],'q2':vec[1],'dup':y_te})\n",
    "dfw2v.dropna(inplace=True)\n",
    "y_te_w2v = dfw2v['dup'].values\n",
    "X_te_w2v = np.hstack((np.array([x for x in dfw2v['q1'].values]),np.array([x for x in dfw2v['q2'].values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=123, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic4 = sklearn.linear_model.LogisticRegression(solver=\"liblinear\", random_state=123)\n",
    "logistic4.fit(X_tr_w2v, y_tr_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND DATASET\n",
    "folder = 'word2vect_pre'\n",
    "# save model\n",
    "if not os.path.isdir(\"model\"):\n",
    "    os.mkdir(\"model\")\n",
    "\n",
    "if not os.path.isdir(f\"model/{folder}_solution\"):\n",
    "        os.mkdir(f\"model/{folder}_solution\")\n",
    "        \n",
    "with open(f\"model/{folder}_solution/{folder}_model.pkl\",'wb') as f:\n",
    "    pickle.dump(logistic4,f)\n",
    "    \n",
    "# save data\n",
    "with open(f'model/{folder}_solution/{folder}_model_X_tr.pkl','wb') as f:\n",
    "    pickle.dump(X_tr_w2v,f)  \n",
    "with open(f'model/{folder}_solution/{folder}_model_X_va.pkl','wb') as f:\n",
    "    pickle.dump(X_va_w2v,f)   \n",
    "with open(f'model/{folder}_solution/{folder}_model_X_te.pkl','wb') as f:\n",
    "    pickle.dump(X_te_w2v,f)\n",
    "with open(f'model/{folder}_solution/{folder}_model_y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr_w2v,f)\n",
    "with open(f'model/{folder}_solution/{folder}_model_y_va.pkl','wb') as f:\n",
    "    pickle.dump(y_va_w2v,f)\n",
    "with open(f'model/{folder}_solution/{folder}_model_y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te_w2v,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25le_Ovw4WYo"
   },
   "source": [
    "### Pre-trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
